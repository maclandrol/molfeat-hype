{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\u2604\ufe0f <code>molfeat-hype</code>","text":""},{"location":"index.html#overview","title":"Overview","text":"<p><code>molfeat-hype</code> is an extension of <code>molfeat</code> that investigates the performance of embeddings from various LLMs trained without explicit molecular context for molecular modeling. It leverages some of the most hyped LLM models in NLP to answer the following question:</p> <p><code>Is it necessary to pretrain/finetune LLMs on molecular context to obtain good molecular representations?</code></p> <p>To find an answer to this question, check out the benchmarks.</p> Spoilers YES! Understanding molecular context/structure/properties is key to building good molecular featurizers."},{"location":"index.html#llms","title":"LLMs:","text":"<p><code>molfeat-hype</code> supports two types of LLM embeddings:</p> <ol> <li> <p>Classic Embeddings: These are classical embeddings provided by foundation models (or any LLMs). The models available in this tool include OpenAI's <code>openai/text-embedding-ada-002</code> model, <code>llama</code>, and several embedding models accessible through <code>sentence-transformers</code>.</p> </li> <li> <p>Instruction-based Embeddings: These are models that have been trained to follow instructions (thus acting like ChatGPT) or are conditional models that require a prompt.</p> <ul> <li>Prompt-based instruction: A model (like Chat-GPT: <code>openai/gpt-3.5-turbo</code>) is asked to act like an all-knowing AI assistant for drug discovery and provide the best molecular representation for the input list of molecules. Here, we parse the representation from the Chat agent output.</li> <li>Conditional embeddings: A model trained for conditional text embeddings that takes instruction as additional input. Here, the embedding is the model underlying representation of the molecule conditioned by the instructions it received. For more information, see this instructor-embedding.</li> </ul> </li> </ol>"},{"location":"index.html#installation","title":"Installation","text":"<p>You can install <code>molfeat-hype</code> using pip. <code>conda</code> installation is planned soon.</p> <pre><code>pip install molfeat-hype\n</code></pre> <p><code>molfeat-hype</code> mostly depends on molfeat and langchain. For a list of complete dependencies, please see the env.yml file.</p>"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>Check out the following projects that made <code>molfeat-hype</code> possible:</p> <ul> <li>To learn more about <code>molfeat</code>, please visit https://molfeat.datamol.io/. To learn more about the plugin system of molfeat, please see extending molfeat.</li> </ul> <ul> <li>Please refer to the <code>langchain</code> documentation for any questions related to langchain.</li> </ul>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>As an open-source project in a rapidly developing field, we are extremely open to contributions, whether in the form of new features, improved infrastructure, or better documentation. For detailed information on how to contribute, see our contribution guide.</p>"},{"location":"index.html#disclaimer","title":"Disclaimer","text":"<p>This repository contains an experimental investigation of LLM embeddings for molecules. Please note that the consistency and usefulness of the returned molecular embeddings are not guaranteed. This project is meant for fun and exploratory purposes only and should not be used as a demonstration of LLM capabilities for molecular embeddings. Any statements made in this repository are the opinions</p>"},{"location":"contribute.html","title":"Contribute","text":"<p>Thank you for your interest in contributing to <code>molfeat-hype</code>! As part of the molfeat community, we welcome all contributions, including code, documentation, and other forms of support. However you choose to contribute, please be mindful and respect our code of conduct.</p>"},{"location":"contribute.html#ways-to-contribute","title":"Ways to contribute","text":"<p>You can contribute to <code>molfeat-hype</code> in several ways:</p> <ul> <li>Fix existing code issues.</li> <li>Submit issues related to bugs or new features.</li> <li>Suggest or implement new featurizers.</li> <li>Improve existing documentation or add new tutorials.</li> </ul> <p>For a more detailed description of the development lifecycle, please refer to the rest of this document.</p>"},{"location":"contribute.html#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>To get started, fork and clone the repository, then install the dependencies. It is strongly recommended that you do so in a new conda environment:</p> <pre><code>mamba env create -n molfeat_hype -f env.yml\nconda activate molfeat_hype\npip install -e .\n</code></pre>"},{"location":"contribute.html#continuous-integration","title":"Continuous Integration","text":"<p><code>molfeat-hype</code> uses Github Actions to:</p> <ul> <li>Build and test the code.</li> <li>Check code formatting using <code>black</code>.</li> <li>Build and deploy documentation for each commit and tag.</li> </ul>"},{"location":"contribute.html#running-tests","title":"Running tests","text":"<p>You can run the tests with:</p> <pre><code>pytest\n</code></pre>"},{"location":"contribute.html#building-the-documentation","title":"Building the documentation","text":"<p>To build and serve the documentation locally, run:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"contribute.html#submitting-pull-requests","title":"Submitting pull requests","text":"<p>If you're considering a large code contribution, please open an issue first to get early feedback on the idea. Once you think the code is ready to be reviewed, push it to your fork and open a pull request. A code owner or maintainer will review your PR and give you feedback.</p>"},{"location":"license.html","title":"License","text":"<pre><code>Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Emmanuel Noutahi\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n</code></pre>"},{"location":"usage.html","title":"Using molfeat-hype","text":""},{"location":"usage.html#usage","title":"Usage","text":"<p>Since <code>molfeat-hype</code> is a <code>molfeat</code> plugin, it follows the same integration principle as with any other <code>molfeat</code> plugin. </p> <p>The following shows examples of how to use the <code>molfeat-hype</code> plugin package automatically when installed.</p> <ol> <li>Using this package directly:</li> </ol> <pre><code>from molfeat_hype.trans.llm_embeddings import LLMTransformer\n\nmol_transf = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\")\n</code></pre> <ol> <li>Enabling autodiscovery as a plugin in <code>molfeat</code>, and addition of all embedding classes as an importable attribute to the entry point group <code>molfeat.trans.pretrained</code>:</li> </ol> <pre><code># Put this somewhere in your code (e.g., in the root __init__ file).\n# Plugins should include any subword of 'molfeat_hype'.\nfrom molfeat.plugins import load_registered_plugins\nload_registered_plugins(add_submodules=True, plugins=[\"hype\"])\n</code></pre> <pre><code># This is now possible everywhere.\nfrom molfeat.trans.pretrained import LLMTransformer\nmol_transf = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\")\n</code></pre> <p>Once you have defined your molecule transformer, use it like any <code>molfeat</code> <code>MoleculeTransformer</code>:</p> <pre><code>import datamol as dm\nsmiles = dm.freesolv()[\"smiles\"].values[:5]\nmol_transf(smiles)\n</code></pre>"},{"location":"usage.html#caching","title":"Caching","text":"<p>We strongly encourage you to set a cache for computing your molecular representation. By default, a temporary <code>in-memory</code> cache will be set and will be purged when the Python runtime is stopped.</p>"},{"location":"usage.html#openai","title":"OpenAI","text":"<p>If you are using the OpenAI embeddings, you need to provide either an <code>open_ai_key</code> argument or define one as an environment variable <code>OPEN_AI_KEY</code>.</p> <p>Please refer to OpenAI's API keys documentation to set up your API keys. You will need to activate billing.</p>"},{"location":"usage.html#llamacpp","title":"llama.cpp","text":"<p>Llama integration requires the llama-cpp-python, which is a Python binding for the llama-cpp package. You will also need the Llama model weights.</p> <p>Do not share any IPFS, magnet links, or any other links to model downloads anywhere in this repository, including in issues, discussions, or pull requests. They will be immediately deleted.</p>"},{"location":"usage.html#warning","title":"Warning","text":"<p>This repository contains an experimental investigation of LLM embeddings for molecules. Please note that the consistency and usefulness of the returned molecular embeddings are not guaranteed. This project is meant for fun and exploratory purposes only, and should not be used as a demonstration of LLM capabilities for molecular embeddings. Any statements made in this repository are the opinions of the authors and do not necessarily reflect the views of any affiliated organizations or individuals. Use at your own risk.</p>"},{"location":"api/molfeat_hype.trans.html","title":"LLM embeddings","text":""},{"location":"api/molfeat_hype.trans.html#classical-embeddings","title":"Classical Embeddings","text":"<p>This section corresponds to classical embedding of a text object (here a molecule in a smiles format)</p>"},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_embeddings.LLMTransformer","title":"<code>LLMTransformer</code>","text":"<p>         Bases: <code>PretrainedMolTransformer</code></p> <p>Large Language Model Embeddings Transformer for molecule. This transformer embeds molecules using available Large Language Models (LLMs) through langchain. Please note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task. They are just trained on a large corpus of text.</p> <p>Caching computation</p> <p>LLMs can be computationally expensive and even financially expensive if you use the OpenAI embeddings. To avoid recomputing the embeddings for the same molecules, we recommend using a molfeat Cache object. By default, an in-memory cache (DataCache) is used, but other caching systems can be explored.</p> Using OpenAI Embeddings <p>If you are using the OpenAI embeddings, you need to provide an 'open_ai_key' argument or define one through an environment variable 'OPEN_AI_KEY'. Please note that only the <code>text-embedding-ada-002</code> model is supported. Refer to OpenAI's documentation for more information.</p> Using LLAMA Embeddings <p>The Llama embeddings are provided via the python bindings of <code>llama.cpp</code>. We do not provide the path to the quantized Llama model. However, it's easy to find them online; some people have shared the torrent/IPFS/direct download links to the Llama weights, then you can quantized them yourself.</p> Using Sentence Transformer Embeddings <p>The sentence transformer embeddings are based on the SentenceTransformers package.</p> Source code in <code>molfeat_hype/trans/llm_embeddings.py</code> <pre><code>class LLMTransformer(PretrainedMolTransformer):\n\"\"\"\n    Large Language Model Embeddings Transformer for molecule.\n    This transformer embeds molecules using available Large Language Models (LLMs) through langchain.\n    Please note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task.\n    They are just trained on a large corpus of text.\n\n\n    !!! warning \"Caching computation\"\n        LLMs can be computationally expensive and even financially expensive if you use the OpenAI embeddings.\n        To avoid recomputing the embeddings for the same molecules, we recommend using a molfeat Cache object.\n        By default, an in-memory cache (DataCache) is used, but other caching systems can be explored.\n\n\n    ??? note \"Using OpenAI Embeddings\"\n        If you are using the OpenAI embeddings, you need to provide an 'open_ai_key' argument or define one through an environment variable 'OPEN_AI_KEY'.\n        Please note that only the `text-embedding-ada-002` model is supported.\n        Refer to OpenAI's documentation for more information.\n\n    ??? note \"Using LLAMA Embeddings\"\n        The Llama embeddings are provided via the python bindings of `llama.cpp`.\n        We do not provide the path to the quantized Llama model. However, it's easy to find them online;\n        some people have shared the torrent/IPFS/direct download links to the Llama weights, then you can quantized them yourself.\n\n    ??? note \"Using Sentence Transformer Embeddings\"\n        The sentence transformer embeddings are based on the SentenceTransformers package.\n\n    \"\"\"\n\n    SUPPORTED_EMBEDDINGS = [\n        \"openai/text-embedding-ada-002\",\n        \"sentence-transformers/all-MiniLM-L6-v2\",\n        \"sentence-transformers/all-mpnet-base-v2\",\n        \"llama.cpp\",\n    ]\n\n    def __init__(\n        self,\n        kind=Union[str, LangChainEmbeddings],\n        standardize: bool = True,\n        precompute_cache: bool = True,\n        n_jobs: int = 0,\n        dtype=float,\n        openai_api_key: Optional[str] = None,\n        quantized_model_path: Optional[str] = None,\n        parallel_kwargs: Optional[dict] = None,\n        **params,\n    ):\n\"\"\"Instantiate a LLM Embeddings transformer\n\n        Args:\n            kind: kind of LLM to use. Supported LLMs are accessible through the SUPPORTED_EMBEDDINGS attribute. Here are a few:\n                - \"openai/text-embedding-ada-002\"\n                - \"sentence-transformers/all-MiniLM-L6-v2\"\n                - \"sentence-transformers/all-mpnet-base-v2\"\n                - \"llama.cpp\"\n                You can also provide any model hosted on hugginface that compute embeddings\n            standardize: if True, standardize smiles before embedding\n            precompute_cache: if True, add a cache to cache the embeddings for the same molecules.\n            n_jobs: number of jobs to use for preprocessing smiles.\n            dtype: data type to use for the embeddings return type\n            openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY\n            quantized_model_path: path to the quantized model for llama.cpp. If None, will try to get it from the environment variable quantized_MODEL_PATH\n            **params: parameters to pass to the LLM embeddings. See langchain documentation\n        \"\"\"\n\n        self.kind = kind\n        self.model = None\n        self.standardize = standardize\n        if isinstance(kind, str):\n            if not kind.startswith(tuple(self.SUPPORTED_EMBEDDINGS)):\n                logger.warning(f\"Model {kind} not found, trying from huggingface hub.\")\n                on_hgf = requests.get(f\"https://huggingface.co/{kind}\")\n                try:\n                    on_hgf.raise_for_status()\n                except:\n                    raise ValueError(\n                        f\"Unknown LLM type {kind} requested. Supported models are {self.SUPPORTED_EMBEDDINGS}\"\n                    )\n            if kind.startswith(\"openai/\"):\n                if openai_api_key is None:\n                    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n                self.model = OpenAIEmbeddings(\n                    model=kind.replace(\"openai/\", \"\"), openai_api_key=openai_api_key, **params\n                )\n            elif kind.startswith(\"llama.cpp\"):\n                if quantized_model_path is None:\n                    quantized_model_path = os.environ.get(\"QUANT_MODEL_PATH\")\n                if quantized_model_path is not None and dm.fs.exists(quantized_model_path):\n                    create_symlink(quantized_model_path, kind)\n                else:\n                    model_base_name = os.path.splitext(os.path.basename(quantized_model_path))[0]\n                    quantized_model_path = dm.fs.glob(dm.fs.join(CACHE_DIR, f\"{model_base_name}*\"))\n                    if len(quantized_model_path) == 0:\n                        raise ValueError(\n                            f\"Could not find the quantized model {model_base_name} anywhere, including in the cache dir {CACHE_DIR}\"\n                        )\n                    quantized_model_path = quantized_model_path[0]\n                with contextlib.redirect_stdout(None):\n                    with contextlib.redirect_stderr(None):\n                        n_ctx = max(params.get(\"n_ctx\", 1024), 1024)\n                        params[\"n_ctx\"] = n_ctx\n                        self.model = LlamaCppEmbeddings(model_path=quantized_model_path, **params)\n                        self.model.client.verbose = False\n            else:\n                self.model = HuggingFaceEmbeddings(\n                    model_name=kind, model_kwargs=params, cache_folder=CACHE_DIR\n                )\n        super().__init__(\n            precompute_cache=precompute_cache,\n            n_jobs=n_jobs,\n            dtype=dtype,\n            device=\"cpu\",\n            parallel_kwargs=parallel_kwargs,\n            **params,\n        )\n\n    def _convert(self, inputs: List[Union[str, dm.Mol]], **kwargs):\n\"\"\"Convert the list of input molecules into the proper format for embeddings\n\n        Args:\n            inputs: list of input molecules\n            **kwargs: additional keyword arguments for API consistency\n\n        \"\"\"\n        self._preload()\n        parallel_kwargs = copy.deepcopy(getattr(self, \"parallel_kwargs\", {}))\n        parallel_kwargs[\"n_jobs\"] = self.n_jobs\n        return convert_smiles(inputs, parallel_kwargs, standardize=self.standardize)\n\n    def _embed(self, smiles: List[str], **kwargs):\n\"\"\"This function takes a list of smiles or molecules and return the featurization\n        corresponding to the inputs.\n        In `transform` and `_transform`, this function is called after calling `_convert`\n\n        Args:\n            smiles: input smiles\n            **kwargs: additional keyword arguments for API consistency\n        \"\"\"\n        return self.model.embed_documents(smiles)\n</code></pre>"},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_embeddings.LLMTransformer.SUPPORTED_EMBEDDINGS","title":"<code>SUPPORTED_EMBEDDINGS = ['openai/text-embedding-ada-002', 'sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-mpnet-base-v2', 'llama.cpp']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_embeddings.LLMTransformer.kind","title":"<code>kind = kind</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_embeddings.LLMTransformer.model","title":"<code>model = LlamaCppEmbeddings(model_path=quantized_model_path, None=params)</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_embeddings.LLMTransformer.standardize","title":"<code>standardize = standardize</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_embeddings.LLMTransformer.__init__","title":"<code>__init__(kind=Union[str, LangChainEmbeddings], standardize=True, precompute_cache=True, n_jobs=0, dtype=float, openai_api_key=None, quantized_model_path=None, parallel_kwargs=None, **params)</code>","text":"<p>Instantiate a LLM Embeddings transformer</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <p>kind of LLM to use. Supported LLMs are accessible through the SUPPORTED_EMBEDDINGS attribute. Here are a few: - \"openai/text-embedding-ada-002\" - \"sentence-transformers/all-MiniLM-L6-v2\" - \"sentence-transformers/all-mpnet-base-v2\" - \"llama.cpp\" You can also provide any model hosted on hugginface that compute embeddings</p> <code>Union[str, LangChainEmbeddings]</code> <code>standardize</code> <code>bool</code> <p>if True, standardize smiles before embedding</p> <code>True</code> <code>precompute_cache</code> <code>bool</code> <p>if True, add a cache to cache the embeddings for the same molecules.</p> <code>True</code> <code>n_jobs</code> <code>int</code> <p>number of jobs to use for preprocessing smiles.</p> <code>0</code> <code>dtype</code> <p>data type to use for the embeddings return type</p> <code>float</code> <code>openai_api_key</code> <code>Optional[str]</code> <p>openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</p> <code>None</code> <code>quantized_model_path</code> <code>Optional[str]</code> <p>path to the quantized model for llama.cpp. If None, will try to get it from the environment variable quantized_MODEL_PATH</p> <code>None</code> <code>**params</code> <p>parameters to pass to the LLM embeddings. See langchain documentation</p> <code>{}</code> Source code in <code>molfeat_hype/trans/llm_embeddings.py</code> <pre><code>def __init__(\n    self,\n    kind=Union[str, LangChainEmbeddings],\n    standardize: bool = True,\n    precompute_cache: bool = True,\n    n_jobs: int = 0,\n    dtype=float,\n    openai_api_key: Optional[str] = None,\n    quantized_model_path: Optional[str] = None,\n    parallel_kwargs: Optional[dict] = None,\n    **params,\n):\n\"\"\"Instantiate a LLM Embeddings transformer\n\n    Args:\n        kind: kind of LLM to use. Supported LLMs are accessible through the SUPPORTED_EMBEDDINGS attribute. Here are a few:\n            - \"openai/text-embedding-ada-002\"\n            - \"sentence-transformers/all-MiniLM-L6-v2\"\n            - \"sentence-transformers/all-mpnet-base-v2\"\n            - \"llama.cpp\"\n            You can also provide any model hosted on hugginface that compute embeddings\n        standardize: if True, standardize smiles before embedding\n        precompute_cache: if True, add a cache to cache the embeddings for the same molecules.\n        n_jobs: number of jobs to use for preprocessing smiles.\n        dtype: data type to use for the embeddings return type\n        openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY\n        quantized_model_path: path to the quantized model for llama.cpp. If None, will try to get it from the environment variable quantized_MODEL_PATH\n        **params: parameters to pass to the LLM embeddings. See langchain documentation\n    \"\"\"\n\n    self.kind = kind\n    self.model = None\n    self.standardize = standardize\n    if isinstance(kind, str):\n        if not kind.startswith(tuple(self.SUPPORTED_EMBEDDINGS)):\n            logger.warning(f\"Model {kind} not found, trying from huggingface hub.\")\n            on_hgf = requests.get(f\"https://huggingface.co/{kind}\")\n            try:\n                on_hgf.raise_for_status()\n            except:\n                raise ValueError(\n                    f\"Unknown LLM type {kind} requested. Supported models are {self.SUPPORTED_EMBEDDINGS}\"\n                )\n        if kind.startswith(\"openai/\"):\n            if openai_api_key is None:\n                openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n            self.model = OpenAIEmbeddings(\n                model=kind.replace(\"openai/\", \"\"), openai_api_key=openai_api_key, **params\n            )\n        elif kind.startswith(\"llama.cpp\"):\n            if quantized_model_path is None:\n                quantized_model_path = os.environ.get(\"QUANT_MODEL_PATH\")\n            if quantized_model_path is not None and dm.fs.exists(quantized_model_path):\n                create_symlink(quantized_model_path, kind)\n            else:\n                model_base_name = os.path.splitext(os.path.basename(quantized_model_path))[0]\n                quantized_model_path = dm.fs.glob(dm.fs.join(CACHE_DIR, f\"{model_base_name}*\"))\n                if len(quantized_model_path) == 0:\n                    raise ValueError(\n                        f\"Could not find the quantized model {model_base_name} anywhere, including in the cache dir {CACHE_DIR}\"\n                    )\n                quantized_model_path = quantized_model_path[0]\n            with contextlib.redirect_stdout(None):\n                with contextlib.redirect_stderr(None):\n                    n_ctx = max(params.get(\"n_ctx\", 1024), 1024)\n                    params[\"n_ctx\"] = n_ctx\n                    self.model = LlamaCppEmbeddings(model_path=quantized_model_path, **params)\n                    self.model.client.verbose = False\n        else:\n            self.model = HuggingFaceEmbeddings(\n                model_name=kind, model_kwargs=params, cache_folder=CACHE_DIR\n            )\n    super().__init__(\n        precompute_cache=precompute_cache,\n        n_jobs=n_jobs,\n        dtype=dtype,\n        device=\"cpu\",\n        parallel_kwargs=parallel_kwargs,\n        **params,\n    )\n</code></pre>"},{"location":"api/molfeat_hype.trans.html#instruct-aware-embeddings","title":"Instruct-aware Embeddings","text":"<p>This section corresponds to models that accept instructions to compute the embedding or an input molecule.</p>"},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.DEFAULT_SYSTEM_PROMPT","title":"<code>DEFAULT_SYSTEM_PROMPT = \"MolAssistant is a powerful large language model trained by the Open Drug Discovery consortium, \\nspecifically designed to assist with various tasks related to drug discovery. Its capabilities range from providing \\nmolecular representation to designing new molecules with specific optimization objectives in mind. Being pre-trained \\non the largest corpus of chemical and biological data, MolAssistant has a deep understanding of chemical structures \\nand extensive biological knowledge. It can generate both human-like text and numerical outputs, providing concise and \\naccurate responses that are coherent with the topic and instructions given.\\n\\nMolAssistant is constantly learning and improving, which allows it to process and understand vast amounts of chemical \\nand biological data, and it can comprehend any molecular structure. The model's knowledge enables it to provide valuable \\ninsights and information for a wide range of tasks, including molecular search, QSAR model building, and molecular generation,\\nmaking it an indispensable tool for advancing the science of molecular modeling and design in drug discovery.\\n\\n{history}\\nHuman: {human_input}\\nMolAssistant:\"</code>  <code>module-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.MODEL_EMBEDDING_INSTRUCTIONS","title":"<code>MODEL_EMBEDDING_INSTRUCTIONS = {'instructor': 'Represent the following molecule for {context}:', 'openai': 'I want you to provide {dimension} dimensional numerical vector at a precision of {precision} as a representation of molecules in the SMILES format that you will receive as input.\\n    You should first start by understanding the chemical structure and electronic properties of the input molecules before generating the {dimension}-dimensional representation for the following task: {context}.\\n    To obtain the output, I will provide you with either a single SMILES command or a list of SMILES commands, and you will reply with the most accurate and informative {dimension}-dimensional representation in a json parseable format where the keys are the molecules and the values their representations.\\n    When generating the output, please ensure that the format is consistent with the task and the instruction given.  Do not write explanations. Do not type anything else unless I instruct you to do so. \\n    In case of any invalid or unrecognized SMILES inputs, please provide a suitable error message. My first molecule is c1ccccc1.'}</code>  <code>module-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer","title":"<code>InstructLLMTransformer</code>","text":"<p>         Bases: <code>PretrainedMolTransformer</code></p> <p>Instruction-following Large Language Model Embeddings Transformer for molecules. This transformer embeds molecules using available LLM through langchain. Note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task. They are just trained on a large corpus of text.</p> Source code in <code>molfeat_hype/trans/llm_instruct_embeddings.py</code> <pre><code>class InstructLLMTransformer(PretrainedMolTransformer):\n\"\"\"\n    Instruction-following Large Language Model Embeddings Transformer for molecules. This transformer embeds molecules using available LLM through langchain.\n    Note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task.\n    They are just trained on a large corpus of text.\n    \"\"\"\n\n    SUPPORTED_EMBEDDINGS = [\n        \"hkunlp/instructor-large\",\n        \"hkunlp/instructor-base\",\n        \"openai/gpt-3.5-turbo\",\n        \"openai/gpt-4\",\n        \"openai/chatgpt\",  # alias for \"openai/gpt-3.5-turbo\"\n    ]\n\n    def __init__(\n        self,\n        kind=Union[str, LangChainEmbeddings],\n        embedding_size: int = 32,\n        context: Optional[str] = \"modelling\",\n        standardize: bool = True,\n        precompute_cache: bool = True,\n        n_jobs: int = 0,\n        conv_buffer_size: int = 10,\n        conv_max_tokens: Optional[int] = None,\n        dtype=float,\n        openai_api_key: Optional[str] = None,\n        precision: int = 5,\n        batch_size: Optional[int] = None,\n        system_prompt: Optional[str] = None,\n        parallel_kwargs: Optional[dict] = None,\n        **params,\n    ):\n\"\"\"Instantiate an instruction following LLM transformer for molecular embeddings\n\n        Args:\n            kind: type or name of the model to use for embeddings\n            embedding_size: size of the embeddings to return for chat-like models\n            context: context to give to the prompt for returning the results. Default is \"modelling\" which is the context for the modelling instructions.\n            standardize: if True, standardize smiles before embedding\n            precompute_cache: if True, add a cache to cache the embeddings for the same molecules.\n            n_jobs: number of jobs to use for preprocessing smiles.\n            conv_buffer_size: conversation buffer size so assistant can remember previous conversations and context for generating features.\n            conv_max_tokens: maximum number of tokens to use for the conversation context. If None, will not use a token size limitations.\n            dtype: data type to use for the embeddings return type\n            openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY\n            precision: float precision of the output vector\n            batch_size: batch size to use for embedding molecules. If None, will not use a batch size\n            system_prompt: system prompt to use for chat-like models. If None, will use the default prompt.\n            **params: parameters to pass to the LLM embeddings. See langchain documentation\n        \"\"\"\n\n        self.kind = kind\n        self.model = None\n        self.standardize = standardize\n        self.context = context or \"modelling\"\n        self.embedding_size = embedding_size\n        self.precision = precision\n        self.batch_size = batch_size\n        self.conv_max_tokens = conv_max_tokens\n        self.conv_buffer_size = conv_buffer_size\n        self._length = None\n        self.system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT\n        params.setdefault(\"temperature\", 0.8)\n        if isinstance(kind, str):\n            if not (\n                kind.startswith(tuple(self.SUPPORTED_EMBEDDINGS)) or kind.startswith(\"openai/\")\n            ):\n                raise ValueError(\n                    f\"Unknown LLM type {kind} requested. Supported models are {self.SUPPORTED_EMBEDDINGS}\"\n                )\n            if kind.startswith(\"openai/\"):\n                if kind == \"openai/chatgpt\":\n                    kind = \"openai/gpt-3.5-turbo\"\n                if openai_api_key is None:\n                    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n                prompt = PromptTemplate(\n                    input_variables=[\"history\", \"human_input\"], template=self.system_prompt\n                )\n                llm = ChatOpenAI(\n                    model_name=kind.replace(\"openai/\", \"\"),\n                    openai_api_key=openai_api_key,\n                    **params,\n                )\n                self.model = LLMChain(\n                    llm=llm,\n                    prompt=prompt,\n                    verbose=False,\n                    memory=EmbeddingConversationMemory(\n                        k=self.conv_buffer_size,\n                        ai_prefix=\"MolAssistant\",\n                        llm=llm,\n                        max_token_limit=self.conv_max_tokens,\n                    ),\n                )\n                output = self.model.predict(\n                    human_input=MODEL_EMBEDDING_INSTRUCTIONS[\"openai\"].format(\n                        dimension=embedding_size, precision=self.precision, context=self.context\n                    )\n                )\n                embeddings = json.loads(output)\n                if \"c1ccccc1\" not in embeddings:\n                    raise ValueError(\n                        \"Model is not able to understand the prompt. Please select a different model\"\n                    )\n                assert (\n                    len(embeddings[\"c1ccccc1\"]) == embedding_size\n                ), \"Model cannot return the correct embedding size.\"\n                self._length = embedding_size\n\n            elif kind.startswith(\"llama.cpp\") or kind.startswith(\"gpt4all\"):\n                raise ValueError(f\"{kind} is not yet supported, because or how slow they are.\")\n            else:\n                # we need to remove temperature key\n                params.pop(\"temperature\", None)\n                self.model = HuggingFaceInstructEmbeddings(\n                    model_name=kind,\n                    embed_instruction=MODEL_EMBEDDING_INSTRUCTIONS[\"instructor\"].format(\n                        context=self.context\n                    ),\n                    model_kwargs=params,\n                    cache_folder=CACHE_DIR,\n                )\n        super().__init__(\n            precompute_cache=precompute_cache,\n            n_jobs=n_jobs,\n            dtype=dtype,\n            device=\"cpu\",\n            parallel_kwargs=parallel_kwargs,\n            **params,\n        )\n\n    def __len__(self):\n\"\"\"Get the length of the featurizer\"\"\"\n        if self._length is not None:\n            return self._length\n        return super().__len__()\n\n    def _convert(self, inputs: List[Union[str, dm.Mol]], **kwargs):\n\"\"\"Convert the list of input molecules into the proper format for embeddings\"\"\"\n        self._preload()\n        parallel_kwargs = copy.deepcopy(getattr(self, \"parallel_kwargs\", {}))\n        parallel_kwargs[\"n_jobs\"] = self.n_jobs\n        return convert_smiles(inputs, parallel_kwargs, standardize=self.standardize)\n\n    def _embed(self, smiles: List[str], **kwargs):\n\"\"\"_embed takes a list of smiles or molecules and return the featurization\n        corresponding to the inputs.  In `transform` and `_transform`, this function is\n        called after calling `_convert`\n\n        Args:\n            smiles: input smiles\n        \"\"\"\n        if isinstance(self.model, LangChainEmbeddings):\n            return self.model.embed_documents(smiles)\n        # basically running embeddings\n        # compute expected total token for inputs based on expected number of char\n        expected_tokens = (self.embedding_size * (self.precision + 5) + 4) * len(smiles) + sum(\n            len(x) + 5 for x in smiles\n        )\n        # we splits the number of smiles to avoid being over the maximum tokens\n        if not self.batch_size:\n            maximum_tokens = self.model.llm.max_tokens or self.model.memory.max_token_limit or 2000\n            n_splits = max(1, int(np.ceil(expected_tokens / maximum_tokens)))\n        else:\n            n_splits = max(1, int(np.ceil(len(smiles) / self.batch_size)))\n        data = {}\n        for batch in tqdm(np.array_split(smiles, n_splits), desc=f\"Batch embedding\", leave=False):\n            json_output = self.model.predict(human_input=\" ,\".join(batch))\n            batch_data = json.loads(json_output)\n            # EN: surprisingly, ChatGPT can return randomized version of a SMILES\n            data.update({dm.unique_id(k.strip()): v for k, v in batch_data.items()})\n        missed_embedding = np.full_like(list(data.values())[0], np.nan)\n        data = [data.get(dm.unique_id(sm), missed_embedding) for sm in smiles]\n        return data\n</code></pre>"},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.SUPPORTED_EMBEDDINGS","title":"<code>SUPPORTED_EMBEDDINGS = ['hkunlp/instructor-large', 'hkunlp/instructor-base', 'openai/gpt-3.5-turbo', 'openai/gpt-4', 'openai/chatgpt']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.context","title":"<code>context = context or 'modelling'</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_buffer_size","title":"<code>conv_buffer_size = conv_buffer_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_max_tokens","title":"<code>conv_max_tokens = conv_max_tokens</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.embedding_size","title":"<code>embedding_size = embedding_size</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.kind","title":"<code>kind = kind</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.model","title":"<code>model = None</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.precision","title":"<code>precision = precision</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.standardize","title":"<code>standardize = standardize</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.system_prompt","title":"<code>system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__init__","title":"<code>__init__(kind=Union[str, LangChainEmbeddings], embedding_size=32, context='modelling', standardize=True, precompute_cache=True, n_jobs=0, conv_buffer_size=10, conv_max_tokens=None, dtype=float, openai_api_key=None, precision=5, batch_size=None, system_prompt=None, parallel_kwargs=None, **params)</code>","text":"<p>Instantiate an instruction following LLM transformer for molecular embeddings</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <p>type or name of the model to use for embeddings</p> <code>Union[str, LangChainEmbeddings]</code> <code>embedding_size</code> <code>int</code> <p>size of the embeddings to return for chat-like models</p> <code>32</code> <code>context</code> <code>Optional[str]</code> <p>context to give to the prompt for returning the results. Default is \"modelling\" which is the context for the modelling instructions.</p> <code>'modelling'</code> <code>standardize</code> <code>bool</code> <p>if True, standardize smiles before embedding</p> <code>True</code> <code>precompute_cache</code> <code>bool</code> <p>if True, add a cache to cache the embeddings for the same molecules.</p> <code>True</code> <code>n_jobs</code> <code>int</code> <p>number of jobs to use for preprocessing smiles.</p> <code>0</code> <code>conv_buffer_size</code> <code>int</code> <p>conversation buffer size so assistant can remember previous conversations and context for generating features.</p> <code>10</code> <code>conv_max_tokens</code> <code>Optional[int]</code> <p>maximum number of tokens to use for the conversation context. If None, will not use a token size limitations.</p> <code>None</code> <code>dtype</code> <p>data type to use for the embeddings return type</p> <code>float</code> <code>openai_api_key</code> <code>Optional[str]</code> <p>openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</p> <code>None</code> <code>precision</code> <code>int</code> <p>float precision of the output vector</p> <code>5</code> <code>batch_size</code> <code>Optional[int]</code> <p>batch size to use for embedding molecules. If None, will not use a batch size</p> <code>None</code> <code>system_prompt</code> <code>Optional[str]</code> <p>system prompt to use for chat-like models. If None, will use the default prompt.</p> <code>None</code> <code>**params</code> <p>parameters to pass to the LLM embeddings. See langchain documentation</p> <code>{}</code> Source code in <code>molfeat_hype/trans/llm_instruct_embeddings.py</code> <pre><code>def __init__(\n    self,\n    kind=Union[str, LangChainEmbeddings],\n    embedding_size: int = 32,\n    context: Optional[str] = \"modelling\",\n    standardize: bool = True,\n    precompute_cache: bool = True,\n    n_jobs: int = 0,\n    conv_buffer_size: int = 10,\n    conv_max_tokens: Optional[int] = None,\n    dtype=float,\n    openai_api_key: Optional[str] = None,\n    precision: int = 5,\n    batch_size: Optional[int] = None,\n    system_prompt: Optional[str] = None,\n    parallel_kwargs: Optional[dict] = None,\n    **params,\n):\n\"\"\"Instantiate an instruction following LLM transformer for molecular embeddings\n\n    Args:\n        kind: type or name of the model to use for embeddings\n        embedding_size: size of the embeddings to return for chat-like models\n        context: context to give to the prompt for returning the results. Default is \"modelling\" which is the context for the modelling instructions.\n        standardize: if True, standardize smiles before embedding\n        precompute_cache: if True, add a cache to cache the embeddings for the same molecules.\n        n_jobs: number of jobs to use for preprocessing smiles.\n        conv_buffer_size: conversation buffer size so assistant can remember previous conversations and context for generating features.\n        conv_max_tokens: maximum number of tokens to use for the conversation context. If None, will not use a token size limitations.\n        dtype: data type to use for the embeddings return type\n        openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY\n        precision: float precision of the output vector\n        batch_size: batch size to use for embedding molecules. If None, will not use a batch size\n        system_prompt: system prompt to use for chat-like models. If None, will use the default prompt.\n        **params: parameters to pass to the LLM embeddings. See langchain documentation\n    \"\"\"\n\n    self.kind = kind\n    self.model = None\n    self.standardize = standardize\n    self.context = context or \"modelling\"\n    self.embedding_size = embedding_size\n    self.precision = precision\n    self.batch_size = batch_size\n    self.conv_max_tokens = conv_max_tokens\n    self.conv_buffer_size = conv_buffer_size\n    self._length = None\n    self.system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT\n    params.setdefault(\"temperature\", 0.8)\n    if isinstance(kind, str):\n        if not (\n            kind.startswith(tuple(self.SUPPORTED_EMBEDDINGS)) or kind.startswith(\"openai/\")\n        ):\n            raise ValueError(\n                f\"Unknown LLM type {kind} requested. Supported models are {self.SUPPORTED_EMBEDDINGS}\"\n            )\n        if kind.startswith(\"openai/\"):\n            if kind == \"openai/chatgpt\":\n                kind = \"openai/gpt-3.5-turbo\"\n            if openai_api_key is None:\n                openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n            prompt = PromptTemplate(\n                input_variables=[\"history\", \"human_input\"], template=self.system_prompt\n            )\n            llm = ChatOpenAI(\n                model_name=kind.replace(\"openai/\", \"\"),\n                openai_api_key=openai_api_key,\n                **params,\n            )\n            self.model = LLMChain(\n                llm=llm,\n                prompt=prompt,\n                verbose=False,\n                memory=EmbeddingConversationMemory(\n                    k=self.conv_buffer_size,\n                    ai_prefix=\"MolAssistant\",\n                    llm=llm,\n                    max_token_limit=self.conv_max_tokens,\n                ),\n            )\n            output = self.model.predict(\n                human_input=MODEL_EMBEDDING_INSTRUCTIONS[\"openai\"].format(\n                    dimension=embedding_size, precision=self.precision, context=self.context\n                )\n            )\n            embeddings = json.loads(output)\n            if \"c1ccccc1\" not in embeddings:\n                raise ValueError(\n                    \"Model is not able to understand the prompt. Please select a different model\"\n                )\n            assert (\n                len(embeddings[\"c1ccccc1\"]) == embedding_size\n            ), \"Model cannot return the correct embedding size.\"\n            self._length = embedding_size\n\n        elif kind.startswith(\"llama.cpp\") or kind.startswith(\"gpt4all\"):\n            raise ValueError(f\"{kind} is not yet supported, because or how slow they are.\")\n        else:\n            # we need to remove temperature key\n            params.pop(\"temperature\", None)\n            self.model = HuggingFaceInstructEmbeddings(\n                model_name=kind,\n                embed_instruction=MODEL_EMBEDDING_INSTRUCTIONS[\"instructor\"].format(\n                    context=self.context\n                ),\n                model_kwargs=params,\n                cache_folder=CACHE_DIR,\n            )\n    super().__init__(\n        precompute_cache=precompute_cache,\n        n_jobs=n_jobs,\n        dtype=dtype,\n        device=\"cpu\",\n        parallel_kwargs=parallel_kwargs,\n        **params,\n    )\n</code></pre>"},{"location":"api/molfeat_hype.trans.html#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the featurizer</p> Source code in <code>molfeat_hype/trans/llm_instruct_embeddings.py</code> <pre><code>def __len__(self):\n\"\"\"Get the length of the featurizer\"\"\"\n    if self._length is not None:\n        return self._length\n    return super().__len__()\n</code></pre>"},{"location":"tutorials/benchmark.html","title":"Benchmark","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 <p>Let's evaluate the effectiveness of the LLM representations against some commonly used representations in Molecular Modelling.</p> <p>We will follow the same setup as the molfeat benchmark. However, it should be noted that this is not an exhaustive benchmark, and therefore the results should be interpreted with caution.</p> <p>The molfeat benchmark employed the following representations: ECFP6, Mordred, and ChemBERTa. To ensure consistency, we will use these same representations and their results.</p> <p>Since LLMs are computationally expensive, we will limit our evaluation to the Lipophilicity benchmark.</p> <p>For our experiments, we will consider the following featurizers:</p> <ul> <li>openai/text-embedding-ada-002: the default OpenAI embedding model.</li> <li>sentence-transformers/all-mpnet-base-v2: a popular sentence embedding model that maps text into a 768-dimensional dense vector.</li> <li>openai/gpt-3.5-turbo: OpenAI's instruction-following model that powers ChatGPT.</li> <li>hkunlp/instructor-large: an instruction-conditioned model for embedding generation.</li> </ul> <p>Tl;dr - Can non-finetuned LLMs outperform hand-crafted or pretrained molecular featurizers?</p> <p> No. Understanding molecular context, structure, and properties is essential for building effective molecular featurizers. However, while it is crucial to comprehend the molecular structure, non-finetuned LLMs still show potential for molecular search. </p> <pre>! pip install auto-sklearn\n</pre> In\u00a0[2]: Copied! <pre>import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport datamol as dm\n\nimport matplotlib.pyplot as plt\nimport autosklearn.classification\nimport autosklearn.regression\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom rdkit.Chem import SaltRemover\n\nfrom sklearn.metrics import mean_absolute_error, roc_auc_score\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom molfeat.utils.cache import FileCache\nfrom molfeat.trans.base import PrecomputedMolTransformer\nfrom molfeat.trans.fp import FPVecTransformer\nfrom molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer\nfrom molfeat_hype.trans.llm_embeddings import LLMTransformer\nfrom molfeat_hype.trans.llm_instruct_embeddings import InstructLLMTransformer\n</pre> import os import warnings import numpy as np import pandas as pd import datamol as dm  import matplotlib.pyplot as plt import autosklearn.classification import autosklearn.regression from tqdm.auto import tqdm from collections import defaultdict from rdkit.Chem import SaltRemover  from sklearn.metrics import mean_absolute_error, roc_auc_score from sklearn.model_selection import GroupShuffleSplit from sklearn.neighbors import KNeighborsClassifier  from molfeat.utils.cache import FileCache from molfeat.trans.base import PrecomputedMolTransformer from molfeat.trans.fp import FPVecTransformer from molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer from molfeat_hype.trans.llm_embeddings import LLMTransformer from molfeat_hype.trans.llm_instruct_embeddings import InstructLLMTransformer <pre>/Users/manu/.miniconda/envs/molfeat_hype/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre># Making the output less verbose\nwarnings.simplefilter(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ndm.disable_rdkit_log()\n</pre> # Making the output less verbose warnings.simplefilter(\"ignore\") os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" dm.disable_rdkit_log() In\u00a0[4]: Copied! <pre>def load_dataset(uri: str, readout_col: str):\n\"\"\"Loads the MoleculeNet dataset\"\"\"\n    df = pd.read_csv(uri)\n    smiles = df[\"smiles\"].values\n    y = df[readout_col].values\n    return smiles, y\n\n\ndef preprocess_smiles(smi):\n\"\"\"Preprocesses the SMILES string\"\"\"\n    mol = dm.to_mol(smi, ordered=True, sanitize=False)    \n    try: \n        mol = dm.sanitize_mol(mol)\n    except:\n        mol = None\n            \n    if mol is None: \n        return\n        \n    mol = dm.standardize_mol(mol, disconnect_metals=True)\n    remover = SaltRemover.SaltRemover()\n    mol = remover.StripMol(mol, dontRemoveEverything=True)\n\n    return dm.to_smiles(mol)\n\n\ndef scaffold_split(smiles):\n\"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\n    scaffolds = [dm.to_smiles(dm.to_scaffold_murcko(dm.to_mol(smi))) for smi in smiles]\n    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    return next(splitter.split(smiles, groups=scaffolds))\n</pre> def load_dataset(uri: str, readout_col: str):     \"\"\"Loads the MoleculeNet dataset\"\"\"     df = pd.read_csv(uri)     smiles = df[\"smiles\"].values     y = df[readout_col].values     return smiles, y   def preprocess_smiles(smi):     \"\"\"Preprocesses the SMILES string\"\"\"     mol = dm.to_mol(smi, ordered=True, sanitize=False)         try:          mol = dm.sanitize_mol(mol)     except:         mol = None                  if mol is None:          return              mol = dm.standardize_mol(mol, disconnect_metals=True)     remover = SaltRemover.SaltRemover()     mol = remover.StripMol(mol, dontRemoveEverything=True)      return dm.to_smiles(mol)   def scaffold_split(smiles):     \"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"     scaffolds = [dm.to_smiles(dm.to_scaffold_murcko(dm.to_mol(smi))) for smi in smiles]     splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)     return next(splitter.split(smiles, groups=scaffolds))  <p>Classic embeddings</p> In\u00a0[5]: Copied! <pre>openai_api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n</pre> openai_api_key = os.environ.get(\"OPENAI_API_KEY\", None) In\u00a0[6]: Copied! <pre>! mkdir -p ../../cache/\n</pre> ! mkdir -p ../../cache/ In\u00a0[7]: Copied! <pre>openai_ada_cache = FileCache(cache_file=\"../../cache/openai_ada_cache.parquet\", name=\"openai_ada_cache\")\ntransf_openai_ada = LLMTransformer(kind=\"openai/text-embedding-ada-002\", openai_api_key=openai_api_key, precompute_cache=openai_ada_cache)\n</pre> openai_ada_cache = FileCache(cache_file=\"../../cache/openai_ada_cache.parquet\", name=\"openai_ada_cache\") transf_openai_ada = LLMTransformer(kind=\"openai/text-embedding-ada-002\", openai_api_key=openai_api_key, precompute_cache=openai_ada_cache) In\u00a0[8]: Copied! <pre>sent_trans_cache = FileCache(cache_file=\"../../cache/sentence_transformer.parquet\", name=\"sent_trans_cache\")\ntransf_sentence = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\", precompute_cache=sent_trans_cache)\n</pre> sent_trans_cache = FileCache(cache_file=\"../../cache/sentence_transformer.parquet\", name=\"sent_trans_cache\") transf_sentence = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\", precompute_cache=sent_trans_cache) <p>Instruct embeddings</p> In\u00a0[9]: Copied! <pre>cond_embed_cache = FileCache(cache_file=\"../../cache/cond_embed.parquet\", name=\"cond_embed_cache\")\ntransf_cond_embed = InstructLLMTransformer(kind=\"hkunlp/instructor-large\", precompute_cache=cond_embed_cache)\n</pre> cond_embed_cache = FileCache(cache_file=\"../../cache/cond_embed.parquet\", name=\"cond_embed_cache\") transf_cond_embed = InstructLLMTransformer(kind=\"hkunlp/instructor-large\", precompute_cache=cond_embed_cache) <pre>load INSTRUCTOR_Transformer\nmax_seq_length  512\n</pre> In\u00a0[10]: Copied! <pre># Prepare the Lipophilicity dataset\nsmiles, y_true = load_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")\n\nsmiles = np.array([preprocess_smiles(smi) for smi in smiles])\nsmiles = np.array([smi for smi in smiles if dm.to_mol(smi) is not None])\n\nfeats_openai_ada, ind_openai_ada = transf_openai_ada(smiles, ignore_errors=True)\n\nX = {\n    \"openai/text-embedding-ada-002\": feats_openai_ada[ind_openai_ada],\n}\n</pre> # Prepare the Lipophilicity dataset smiles, y_true = load_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")  smiles = np.array([preprocess_smiles(smi) for smi in smiles]) smiles = np.array([smi for smi in smiles if dm.to_mol(smi) is not None])  feats_openai_ada, ind_openai_ada = transf_openai_ada(smiles, ignore_errors=True)  X = {     \"openai/text-embedding-ada-002\": feats_openai_ada[ind_openai_ada], } In\u00a0[11]: Copied! <pre>feats_sentence, ind_sentence = transf_sentence(smiles, ignore_errors=True)\nX[\"sentence-transformers/all-mpnet-base-v2\"] = feats_sentence[ind_sentence]\n</pre> feats_sentence, ind_sentence = transf_sentence(smiles, ignore_errors=True) X[\"sentence-transformers/all-mpnet-base-v2\"] = feats_sentence[ind_sentence] In\u00a0[12]: Copied! <pre># feats_cond_embed = transf_cond_embed.batch_transform(transf_cond_embed, smiles, batch_size=512, n_jobs=8, progress=True)\n# ind_cond_embed = np.arange(len(smiles))\n# X[\"hkunlp/instructor-large\"] = feats_cond_embed[ind_cond_embed]\n</pre> # feats_cond_embed = transf_cond_embed.batch_transform(transf_cond_embed, smiles, batch_size=512, n_jobs=8, progress=True) # ind_cond_embed = np.arange(len(smiles)) # X[\"hkunlp/instructor-large\"] = feats_cond_embed[ind_cond_embed] In\u00a0[13]: Copied! <pre># mols = [dm.to_mol(smi) for smi in smiles]\n# _cache = dict(zip(mols, feats_cond_embed))\n# transf_cond_embed.precompute_cache.update(_cache)\n</pre> # mols = [dm.to_mol(smi) for smi in smiles] # _cache = dict(zip(mols, feats_cond_embed)) # transf_cond_embed.precompute_cache.update(_cache) In\u00a0[14]: Copied! <pre>feats_cond_embed, ind_cond_embed = transf_cond_embed(smiles, ignore_errors=True)\nX[\"hkunlp/instructor-large\"] = feats_cond_embed[ind_cond_embed]\n</pre> feats_cond_embed, ind_cond_embed = transf_cond_embed(smiles, ignore_errors=True) X[\"hkunlp/instructor-large\"] = feats_cond_embed[ind_cond_embed] In\u00a0[15]: Copied! <pre># transf_base_chatgpt = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16, openai_api_key=openai_api_key, precompute_cache=False, conv_buffer_size=4, request_timeout=300)\n# transf_chatgpt = PrecomputedMolTransformer(cache=chatgpt_cache, featurizer=transf_base_chatgpt)\n#feats_chatgpt = transf_chatgpt.batch_transform(transf_chatgpt, smiles, batch_size=16, n_jobs=-1)\n#X[\"openai/chatgpt\"] = feats_chatgpt[ind_chatgpt]\n# chatgpt_cache.update(transf_chatgpt.cache)\n# chatgpt_cache.save_to_file()\n</pre> # transf_base_chatgpt = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16, openai_api_key=openai_api_key, precompute_cache=False, conv_buffer_size=4, request_timeout=300) # transf_chatgpt = PrecomputedMolTransformer(cache=chatgpt_cache, featurizer=transf_base_chatgpt) #feats_chatgpt = transf_chatgpt.batch_transform(transf_chatgpt, smiles, batch_size=16, n_jobs=-1) #X[\"openai/chatgpt\"] = feats_chatgpt[ind_chatgpt] # chatgpt_cache.update(transf_chatgpt.cache) # chatgpt_cache.save_to_file() In\u00a0[16]: Copied! <pre>chatgpt_cache = FileCache(cache_file=\"../../cache/chatgpt.parquet\", name=\"chatgpt_cache\")\ntransf_chatgpt = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16, openai_api_key=openai_api_key, conv_buffer_size=3, request_timeout=300, precompute_cache=chatgpt_cache, batch_size=4)\n</pre> chatgpt_cache = FileCache(cache_file=\"../../cache/chatgpt.parquet\", name=\"chatgpt_cache\") transf_chatgpt = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16, openai_api_key=openai_api_key, conv_buffer_size=3, request_timeout=300, precompute_cache=chatgpt_cache, batch_size=4) In\u00a0[17]: Copied! <pre># for k, x in transf_chatgpt.precompute_cache.cache.copy().items():\n#     if x is None  or np.any(np.isnan(x)):\n#         del transf_chatgpt.precompute_cache.cache[k]\n</pre> # for k, x in transf_chatgpt.precompute_cache.cache.copy().items(): #     if x is None  or np.any(np.isnan(x)): #         del transf_chatgpt.precompute_cache.cache[k] In\u00a0[18]: Copied! <pre>feats_chatgpt, ind_chatgpt = transf_chatgpt(smiles, ignore_errors=True)\nX[\"openai/chatgpt\"] = feats_chatgpt#[ind_chatgpt]\n</pre> feats_chatgpt, ind_chatgpt = transf_chatgpt(smiles, ignore_errors=True) X[\"openai/chatgpt\"] = feats_chatgpt#[ind_chatgpt] In\u00a0[19]: Copied! <pre># transf_sentence.precompute_cache.save_to_file()\n# transf_openai_ada.precompute_cache.save_to_file()\n# transf_cond_embed.precompute_cache.save_to_file()\n# transf_chatgpt.precompute_cache.save_to_file()\n</pre> # transf_sentence.precompute_cache.save_to_file() # transf_openai_ada.precompute_cache.save_to_file() # transf_cond_embed.precompute_cache.save_to_file() # transf_chatgpt.precompute_cache.save_to_file() In\u00a0[20]: Copied! <pre># Train a model\ntrain_ind, test_ind = scaffold_split(smiles)\n\nlipo_scores = {}\nfor name, feats in tqdm(X.items()):\n    # print(name, feats.shape, y_true.shape, np.any(np.isnan(feats)))\n    # Train\n    automl = autosklearn.regression.AutoSklearnRegressor(\n        memory_limit=None, \n        # For practicality\u2019s sake, limit this to 5 minutes! \n        # (x3 = 15 min in total)\n        time_left_for_this_task=360,  \n        n_jobs=-1,\n        seed=1,\n    )\n    automl.fit(feats[train_ind], y_true[train_ind])\n    \n    \n    # Predict and evaluate\n    y_hat = automl.predict(feats[test_ind])\n    \n    # Evaluate\n    mae = mean_absolute_error(y_true[test_ind], y_hat)\n    lipo_scores[name] = mae\n\nlipo_scores\n</pre> # Train a model train_ind, test_ind = scaffold_split(smiles)  lipo_scores = {} for name, feats in tqdm(X.items()):     # print(name, feats.shape, y_true.shape, np.any(np.isnan(feats)))     # Train     automl = autosklearn.regression.AutoSklearnRegressor(         memory_limit=None,          # For practicality\u2019s sake, limit this to 5 minutes!          # (x3 = 15 min in total)         time_left_for_this_task=360,           n_jobs=-1,         seed=1,     )     automl.fit(feats[train_ind], y_true[train_ind])               # Predict and evaluate     y_hat = automl.predict(feats[test_ind])          # Evaluate     mae = mean_absolute_error(y_true[test_ind], y_hat)     lipo_scores[name] = mae  lipo_scores <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>[WARNING] [2023-05-04 13:13:47,343:Client-AutoML(1):ffcb4af8-ea9e-11ed-b288-acde48001122] Capping the per_run_time_limit to 178.0 to have time for a least 2 models in each process.\n</pre> <pre> 25%|\u2588\u2588\u258c       | 1/4 [06:18&lt;18:54, 378.18s/it]</pre> <pre>[WARNING] [2023-05-04 13:19:55,515:Client-AutoML(1):e158faba-ea9f-11ed-b288-acde48001122] Capping the per_run_time_limit to 178.0 to have time for a least 2 models in each process.\n</pre> <pre> 50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [12:18&lt;12:15, 367.70s/it]</pre> <pre>[WARNING] [2023-05-04 13:25:54,854:Client-AutoML(1):b80fdbaa-eaa0-11ed-b288-acde48001122] Capping the per_run_time_limit to 178.0 to have time for a least 2 models in each process.\n</pre> <pre> 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [18:19&lt;06:04, 364.73s/it]</pre> <pre>[WARNING] [2023-05-04 13:31:55,652:Client-AutoML(1):8f2c8d7c-eaa1-11ed-b288-acde48001122] Capping the per_run_time_limit to 178.0 to have time for a least 2 models in each process.\n[WARNING] [2023-05-04 13:32:16,838:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:32:23,216:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:32:23,664:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:32:31,623:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:32:37,678:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:33:48,003:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:33:54,291:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:34:01,525:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:34:04,300:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:34:06,781:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:34:09,329:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:34:59,301:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:34:59,813:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:35:04,333:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:35:06,000:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:35:06,373:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:35:09,862:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n[WARNING] [2023-05-04 13:35:13,636:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n\tModels besides current dummy model: 0\n\tDummy models: 1\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [24:56&lt;00:00, 374.17s/it]\n</pre> Out[20]: <pre>{'openai/text-embedding-ada-002': 0.8497511191899347,\n 'sentence-transformers/all-mpnet-base-v2': 0.846618632301065,\n 'hkunlp/instructor-large': 0.8109557972763406,\n 'openai/chatgpt': 0.9247671337879687}</pre> In\u00a0[22]: Copied! <pre>## Let's use a random baseline predicting the mean/median of the dataset\n\nfrom sklearn.dummy import DummyRegressor\nrandom_dummy_regr = DummyRegressor(strategy=\"mean\")\nrandom_dummy_regr.fit(feats[train_ind], y_true[train_ind])\ny_random_pred = random_dummy_regr.predict(feats[test_ind])\nmae = mean_absolute_error(y_true[test_ind], y_random_pred)\nlipo_scores[\"dummy\"] = mae\nmae\n</pre> ## Let's use a random baseline predicting the mean/median of the dataset  from sklearn.dummy import DummyRegressor random_dummy_regr = DummyRegressor(strategy=\"mean\") random_dummy_regr.fit(feats[train_ind], y_true[train_ind]) y_random_pred = random_dummy_regr.predict(feats[test_ind]) mae = mean_absolute_error(y_true[test_ind], y_random_pred) lipo_scores[\"dummy\"] = mae mae Out[22]: <pre>0.9226949930564216</pre> In\u00a0[50]: Copied! <pre>from scipy.spatial import distance\n</pre> from scipy.spatial import distance In\u00a0[19]: Copied! <pre># Setup the featurizers\ntrans_ecfp = FPVecTransformer(kind=\"ecfp:6\", n_jobs=-1)\ntrans_chemberta = PretrainedHFTransformer(kind='ChemBERTa-77M-MLM', notation='smiles')\n</pre> # Setup the featurizers trans_ecfp = FPVecTransformer(kind=\"ecfp:6\", n_jobs=-1) trans_chemberta = PretrainedHFTransformer(kind='ChemBERTa-77M-MLM', notation='smiles') In\u00a0[21]: Copied! <pre>ecfp_embeddings = trans_ecfp(smiles)\nchemberta_embeddings = trans_chemberta.batch_transform(trans_chemberta, smiles, concatenate=True, batch_size=128)\n</pre> ecfp_embeddings = trans_ecfp(smiles) chemberta_embeddings = trans_chemberta.batch_transform(trans_chemberta, smiles, concatenate=True, batch_size=128) In\u00a0[27]: Copied! <pre>ecfp_mat = distance.pdist(ecfp_embeddings, metric=\"cosine\")\nchemberta_mat = distance.pdist(chemberta_embeddings, metric=\"cosine\")\nchatgpt_mat = distance.pdist(X[\"openai/chatgpt\"], metric=\"cosine\")\nst_mat = distance.pdist(X[\"sentence-transformers/all-mpnet-base-v2\"], metric=\"cosine\")\nopenai_mat = distance.pdist(X[\"openai/text-embedding-ada-002\"], metric=\"cosine\")\ninstruct_mat = distance.pdist(X[\"hkunlp/instructor-large\"], metric=\"cosine\")\n</pre> ecfp_mat = distance.pdist(ecfp_embeddings, metric=\"cosine\") chemberta_mat = distance.pdist(chemberta_embeddings, metric=\"cosine\") chatgpt_mat = distance.pdist(X[\"openai/chatgpt\"], metric=\"cosine\") st_mat = distance.pdist(X[\"sentence-transformers/all-mpnet-base-v2\"], metric=\"cosine\") openai_mat = distance.pdist(X[\"openai/text-embedding-ada-002\"], metric=\"cosine\") instruct_mat = distance.pdist(X[\"hkunlp/instructor-large\"], metric=\"cosine\") In\u00a0[30]: Copied! <pre># transform into square matrix\necfp_square_mat = distance.squareform(ecfp_mat)\nchatgpt_square_mat = distance.squareform(chatgpt_mat)\nchemberta_square_mat = distance.squareform(chemberta_mat)\nst_square_mat = distance.squareform(st_mat)\nopenai_square_mat = distance.squareform(openai_mat)\ninstruct_square_mat = distance.squareform(instruct_mat)\n</pre> # transform into square matrix ecfp_square_mat = distance.squareform(ecfp_mat) chatgpt_square_mat = distance.squareform(chatgpt_mat) chemberta_square_mat = distance.squareform(chemberta_mat) st_square_mat = distance.squareform(st_mat) openai_square_mat = distance.squareform(openai_mat) instruct_square_mat = distance.squareform(instruct_mat) In\u00a0[65]: Copied! <pre>def plot_mol_analogs(mol_ind=1):\n    ecfp_selection = np.argsort(ecfp_square_mat, axis=-1)[:, :5][mol_ind]\n    chatgpt_selection = np.argsort(chatgpt_square_mat, axis=-1)[:, :5][mol_ind]\n    chemberta_selection = np.argsort(chemberta_square_mat, axis=-1)[:, :5][mol_ind]\n    st_selection = np.argsort(st_square_mat, axis=-1)[:, :5][mol_ind]\n    openai_selection = np.argsort(openai_square_mat, axis=-1)[:, :5][mol_ind]\n    instruct_selection = np.argsort(instruct_square_mat, axis=-1)[:, :5][mol_ind]\n    out = dm.to_image(\n        [dm.to_mol(smiles[i]) for i in ecfp_selection]+\n        [dm.to_mol(smiles[i]) for i in chemberta_selection]+\n        [dm.to_mol(smiles[i]) for i in st_selection]+\n        [dm.to_mol(smiles[i]) for i in openai_selection]+\n        [dm.to_mol(smiles[i]) for i in chatgpt_selection]+\n        [dm.to_mol(smiles[i]) for i in instruct_selection],\n        legends=[\"Query\"]+[\"ECFP6\"]*4+[\"Query\"]+[\"ChemBERTa\"]*4+[\"Query\"]+[\"Sentence Transformers\"]*4+[\"Query\"]+[\"OpenAI-Embeddings\"]*4 +[\"Query\"]+[\"ChatGPT\"]*4+[\"Query\"]+[\"Instructor\"]*4,\n        n_cols=5,\n        mol_size=(200, 150),\n        )\n    return out\n</pre> def plot_mol_analogs(mol_ind=1):     ecfp_selection = np.argsort(ecfp_square_mat, axis=-1)[:, :5][mol_ind]     chatgpt_selection = np.argsort(chatgpt_square_mat, axis=-1)[:, :5][mol_ind]     chemberta_selection = np.argsort(chemberta_square_mat, axis=-1)[:, :5][mol_ind]     st_selection = np.argsort(st_square_mat, axis=-1)[:, :5][mol_ind]     openai_selection = np.argsort(openai_square_mat, axis=-1)[:, :5][mol_ind]     instruct_selection = np.argsort(instruct_square_mat, axis=-1)[:, :5][mol_ind]     out = dm.to_image(         [dm.to_mol(smiles[i]) for i in ecfp_selection]+         [dm.to_mol(smiles[i]) for i in chemberta_selection]+         [dm.to_mol(smiles[i]) for i in st_selection]+         [dm.to_mol(smiles[i]) for i in openai_selection]+         [dm.to_mol(smiles[i]) for i in chatgpt_selection]+         [dm.to_mol(smiles[i]) for i in instruct_selection],         legends=[\"Query\"]+[\"ECFP6\"]*4+[\"Query\"]+[\"ChemBERTa\"]*4+[\"Query\"]+[\"Sentence Transformers\"]*4+[\"Query\"]+[\"OpenAI-Embeddings\"]*4 +[\"Query\"]+[\"ChatGPT\"]*4+[\"Query\"]+[\"Instructor\"]*4,         n_cols=5,         mol_size=(200, 150),         )     return out In\u00a0[66]: Copied! <pre>plot_mol_analogs(1)\n</pre> plot_mol_analogs(1) Out[66]: In\u00a0[67]: Copied! <pre>plot_mol_analogs(80)\n</pre> plot_mol_analogs(80) Out[67]: In\u00a0[68]: Copied! <pre>plot_mol_analogs(3500)\n</pre> plot_mol_analogs(3500) Out[68]:"},{"location":"tutorials/benchmark.html#lipophilicity-benchmark","title":"Lipophilicity benchmark\u00b6","text":""},{"location":"tutorials/benchmark.html#lipophilicity","title":"Lipophilicity\u00b6","text":"<p>Lipophilicity is a regression task with 4200 molecules</p>"},{"location":"tutorials/benchmark.html#conclusion","title":"Conclusion\u00b6","text":"Dataset Metric Representation Score Rank Lipophilicity MAE \u2193 Mordred 0.579 0 ECFP 0.727 1 ChemBERTa 0.740 2 hkunlp/instructor-large 0.811 3 sentence-transformers/all-mpnet-base-v2 0.847 4 openai/text-embedding-ada-002 0.850 5 Dummy (mean) 0.922 6 openai/chatgpt 0.924 7 <p>As expected, the molecular structure/context/properties aware featurizers, such as <code>ECFP6</code>, <code>Mordred</code> and <code>ChemBERTa</code>, outperformed the models built with non-finetuned LLM embeddings. Some of the LLM-based models even performed worse than what you would expect from a random models predicting the average of the output distribution. However, it is worth noting that the instruction-conditioned model, <code>hkunlp/instructor-large</code>, performed reasonably well, while <code>ChatGPT</code> had the lowest performance among all the models tested.</p>"},{"location":"tutorials/benchmark.html#investigating-similarity-search","title":"Investigating similarity search\u00b6","text":"<p>Ok, so LLMs are not very good for modelling, but how good are they for  analogs search ?</p>"},{"location":"tutorials/benchmark.html#conclusion","title":"Conclusion\u00b6","text":"<p>The results are slightly surprising. Except for ChatGPT who struggles a lot, most LLMs, embeddings are able to capture \"some\" molecular context and could be interesting to explore for <code>scaffold hopping</code> for example.</p> <p>On ChatGPT, maybe improving the prompt could help.</p>"},{"location":"tutorials/getting-started.html","title":"Getting Started","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># avoid annoying tokenizer warnings\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n</pre> # avoid annoying tokenizer warnings import os os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" In\u00a0[3]: Copied! <pre>import datamol as dm\nsmiles = dm.freesolv()[\"smiles\"].values[:5]\nsmiles\n</pre> import datamol as dm smiles = dm.freesolv()[\"smiles\"].values[:5] smiles Out[3]: <pre>array(['CN(C)C(=O)c1ccc(cc1)OC', 'CS(=O)(=O)Cl', 'CC(C)C=C', 'CCc1cnccn1',\n       'CCCCCCCO'], dtype=object)</pre> <p>In these examples we will explore various embeddings provided by the <code>molfeat-hype</code> plugin of <code>molfeat</code>. We are interested in understanding and assessing how good Large Language Models (LLMs) that have NOT been trained or finetuned with any particular molecular context can get on molecular featurization.</p> In\u00a0[4]: Copied! <pre>from molfeat_hype.trans.llm_embeddings import LLMTransformer\n</pre> from molfeat_hype.trans.llm_embeddings import LLMTransformer <pre>/Users/manu/.miniconda/envs/molfeat_hype/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[5]: Copied! <pre>embedder = LLMTransformer(kind=\"openai/text-embedding-ada-002\")\nout = embedder(smiles)\nout.shape\n</pre> embedder = LLMTransformer(kind=\"openai/text-embedding-ada-002\") out = embedder(smiles) out.shape Out[5]: <pre>(5, 1536)</pre> In\u00a0[6]: Copied! <pre>len(embedder)\n</pre> len(embedder) Out[6]: <pre>1536</pre> In\u00a0[7]: Copied! <pre># the cache should have this molecule\nlen(embedder.precompute_cache.get(\"CCCCCCCO\"))\n</pre> # the cache should have this molecule len(embedder.precompute_cache.get(\"CCCCCCCO\")) Out[7]: <pre>1536</pre> In\u00a0[8]: Copied! <pre>embedder = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\")\nout = embedder(smiles)\nout.shape\n</pre> embedder = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\") out = embedder(smiles) out.shape Out[8]: <pre>(5, 768)</pre> In\u00a0[9]: Copied! <pre># Path to the llama quantized weight. \n# You can find them online by asking Meta. \n# Someone also said there is a torrent/IPFS/direct download somewhere of the original llama weight\n# After getting the llama weight, you can quantized them yourself.\nlama_quantized_model_path = \"/Users/manu/Code/llama.cpp/models/7B/ggml-model-q4_0.bin\"\n</pre> # Path to the llama quantized weight.  # You can find them online by asking Meta.  # Someone also said there is a torrent/IPFS/direct download somewhere of the original llama weight # After getting the llama weight, you can quantized them yourself. lama_quantized_model_path = \"/Users/manu/Code/llama.cpp/models/7B/ggml-model-q4_0.bin\" In\u00a0[10]: Copied! <pre>embedder = LLMTransformer(kind=\"llama.cpp\", quantized_model_path=lama_quantized_model_path)\nout = embedder(smiles)\nout.shape\n</pre> embedder = LLMTransformer(kind=\"llama.cpp\", quantized_model_path=lama_quantized_model_path) out = embedder(smiles) out.shape <pre>llama.cpp: loading model from /Users/manu/Code/llama.cpp/models/7B/ggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 1024\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: n_parts    = 1\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =  59.11 KB\nllama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\nllama_init_from_file: kv self size  = 1024.00 MB\n</pre> Out[10]: <pre>(5, 4096)</pre> In\u00a0[11]: Copied! <pre>from molfeat_hype.trans.llm_instruct_embeddings import InstructLLMTransformer\n</pre> from molfeat_hype.trans.llm_instruct_embeddings import InstructLLMTransformer In\u00a0[17]: Copied! <pre># should fail if the model did not understand the prompt\nembedder = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16)\n</pre> # should fail if the model did not understand the prompt embedder = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16) <pre>2023-04-30 19:21:26.148 | WARNING  | molfeat.trans.base:__init__:51 - The 'InstructLLMTransformer' interaction has been superseded by a new class with id 0x7fdbfd704090\n</pre> In\u00a0[18]: Copied! <pre>out = embedder(smiles)\nout.shape\n</pre> out = embedder(smiles) out.shape Out[18]: <pre>(5, 16)</pre> In\u00a0[14]: Copied! <pre># should fail if the model did not understand the prompt\n# we recommend the instructor-large model\nembedder = InstructLLMTransformer(kind=\"hkunlp/instructor-large\")\nout = embedder(smiles)\nout.shape\n</pre> # should fail if the model did not understand the prompt # we recommend the instructor-large model embedder = InstructLLMTransformer(kind=\"hkunlp/instructor-large\") out = embedder(smiles) out.shape <pre>load INSTRUCTOR_Transformer\nmax_seq_length  512\n</pre> Out[14]: <pre>(5, 768)</pre>"},{"location":"tutorials/getting-started.html#classic-embeddings","title":"Classic Embeddings\u00b6","text":"<p>Classic embeddings are embeddings provided by a LLM directly.</p>"},{"location":"tutorials/getting-started.html#using-the-openai-api-for-embeddings","title":"Using the OPENAI API for embeddings\u00b6","text":""},{"location":"tutorials/getting-started.html#using-the-sentence-transformers-models","title":"Using the Sentence-Transformers models\u00b6","text":""},{"location":"tutorials/getting-started.html#using-the-llama-weights","title":"Using the Llama weights\u00b6","text":"<p>To use the Llama weights, you need to obtain them first, then follow the instruction provided in the llama.cpp repo to get 4-bits quantization of model weights.</p>"},{"location":"tutorials/getting-started.html#instruction-based-models","title":"Instruction-based models\u00b6","text":"<p><code>molfeat_hype</code> provides two types of instruction-based models for molecule embeddings:</p> <ol> <li><p>Prompt-based instruction: a ChatGPT model is trained to act like an all-knowing assistant for drug discovery, providing the best molecular representation for the input list of molecules. The representation is parsed from the Chat agent output.</p> </li> <li><p>Conditional embedding: a model trained for conditional text embeddings that takes instructions in its input. The embedding is the model's underlying representation of the molecule, conditioned by the instructions it received. For more information, see instructor-embedding.</p> </li> </ol>"},{"location":"tutorials/getting-started.html#using-the-chatgpt-embeddings","title":"Using the ChatGPT embeddings\u00b6","text":""},{"location":"tutorials/getting-started.html#using-the-instructor-embeddings","title":"Using the instructor embeddings\u00b6","text":""}]}