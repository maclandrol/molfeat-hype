
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="molfeat plugin that leverages the most hyped LLM models in NLP for molecular featurization">
      
      
      
      
        <link rel="prev" href="../tutorials/benchmark.html">
      
      
        <link rel="next" href="../license.html">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.8">
    
    
      
        <title>molfeat_hype.trans - molfeat-hype</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.ded33207.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llm-embeddings" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="molfeat-hype" class="md-header__button md-logo" aria-label="molfeat-hype" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            molfeat-hype
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              molfeat_hype.trans
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/maclandrol/molfeat-hype" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    maclandrol/molfeat-hype
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="molfeat-hype" class="md-nav__button md-logo" aria-label="molfeat-hype" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    molfeat-hype
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/maclandrol/molfeat-hype" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    maclandrol/molfeat-hype
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Why ?
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../usage.html" class="md-nav__link">
        Using molfeat-hype
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/getting-started.html" class="md-nav__link">
        Getting Started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/benchmark.html" class="md-nav__link">
        Benchmark
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          molfeat_hype.trans
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="molfeat_hype.trans.html" class="md-nav__link md-nav__link--active">
        molfeat_hype.trans
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classical-embeddings" class="md-nav__link">
    Classical Embeddings
  </a>
  
    <nav class="md-nav" aria-label="Classical Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings" class="md-nav__link">
    molfeat_hype.trans.llm_embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer" class="md-nav__link">
    LLMTransformer
  </a>
  
    <nav class="md-nav" aria-label="LLMTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.SUPPORTED_EMBEDDINGS" class="md-nav__link">
    SUPPORTED_EMBEDDINGS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.kind" class="md-nav__link">
    kind
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.standardize" class="md-nav__link">
    standardize
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instruct-aware-embeddings" class="md-nav__link">
    Instruct-aware Embeddings
  </a>
  
    <nav class="md-nav" aria-label="Instruct-aware Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings" class="md-nav__link">
    molfeat_hype.trans.llm_instruct_embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.DEFAULT_SYSTEM_PROMPT" class="md-nav__link">
    DEFAULT_SYSTEM_PROMPT
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.MODEL_EMBEDDING_INSTRUCTIONS" class="md-nav__link">
    MODEL_EMBEDDING_INSTRUCTIONS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer" class="md-nav__link">
    InstructLLMTransformer
  </a>
  
    <nav class="md-nav" aria-label="InstructLLMTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.SUPPORTED_EMBEDDINGS" class="md-nav__link">
    SUPPORTED_EMBEDDINGS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.batch_size" class="md-nav__link">
    batch_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.context" class="md-nav__link">
    context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_buffer_size" class="md-nav__link">
    conv_buffer_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_max_tokens" class="md-nav__link">
    conv_max_tokens
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.embedding_size" class="md-nav__link">
    embedding_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.kind" class="md-nav__link">
    kind
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.precision" class="md-nav__link">
    precision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.standardize" class="md-nav__link">
    standardize
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.system_prompt" class="md-nav__link">
    system_prompt
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../license.html" class="md-nav__link">
        License
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classical-embeddings" class="md-nav__link">
    Classical Embeddings
  </a>
  
    <nav class="md-nav" aria-label="Classical Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings" class="md-nav__link">
    molfeat_hype.trans.llm_embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer" class="md-nav__link">
    LLMTransformer
  </a>
  
    <nav class="md-nav" aria-label="LLMTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.SUPPORTED_EMBEDDINGS" class="md-nav__link">
    SUPPORTED_EMBEDDINGS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.kind" class="md-nav__link">
    kind
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.standardize" class="md-nav__link">
    standardize
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instruct-aware-embeddings" class="md-nav__link">
    Instruct-aware Embeddings
  </a>
  
    <nav class="md-nav" aria-label="Instruct-aware Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings" class="md-nav__link">
    molfeat_hype.trans.llm_instruct_embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.DEFAULT_SYSTEM_PROMPT" class="md-nav__link">
    DEFAULT_SYSTEM_PROMPT
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.MODEL_EMBEDDING_INSTRUCTIONS" class="md-nav__link">
    MODEL_EMBEDDING_INSTRUCTIONS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer" class="md-nav__link">
    InstructLLMTransformer
  </a>
  
    <nav class="md-nav" aria-label="InstructLLMTransformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.SUPPORTED_EMBEDDINGS" class="md-nav__link">
    SUPPORTED_EMBEDDINGS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.batch_size" class="md-nav__link">
    batch_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.context" class="md-nav__link">
    context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_buffer_size" class="md-nav__link">
    conv_buffer_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_max_tokens" class="md-nav__link">
    conv_max_tokens
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.embedding_size" class="md-nav__link">
    embedding_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.kind" class="md-nav__link">
    kind
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.precision" class="md-nav__link">
    precision
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.standardize" class="md-nav__link">
    standardize
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.system_prompt" class="md-nav__link">
    system_prompt
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="llm-embeddings">LLM embeddings<a class="headerlink" href="#llm-embeddings" title="Permanent link">&para;</a></h1>
<h2 id="classical-embeddings">Classical Embeddings<a class="headerlink" href="#classical-embeddings" title="Permanent link">&para;</a></h2>
<p>This section corresponds to classical embedding of a text object (here a molecule in a smiles format)</p>


<div class="doc doc-object doc-module">


<a id="molfeat_hype.trans.llm_embeddings"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="molfeat_hype.trans.llm_embeddings.LLMTransformer" class="doc doc-heading">
        <code>LLMTransformer</code>


<a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="molfeat.trans.pretrained.base.PretrainedMolTransformer">PretrainedMolTransformer</span></code></p>

  
      <p>Large Language Model Embeddings Transformer for molecule. This transformer embeds molecules using available LLM through langchain.
Note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task.
They are just trained on a large corpus of text.</p>
<div class="admonition warning">
<p class="admonition-title">Caching computation</p>
<p>LLMs can be computationally expensive and even financially expensive if you use the OpenAI embeddings.
To avoid recomputing the embeddings for the same molecules, we recommend using a molfeat Cache object.
By default an in-memory  cache (DataCache) is used, but please explore other caching systems.</p>
</div>
<details class="note">
<summary>Using OpenAI Embeddings</summary>
<p>If you are using the OpenAI embeddings, you need to either provide a 'open_ai_key'
argument or define one as an environment variable 'OPEN_AI_KEY'.
Please note that only the <code>text-embedding-ada-002</code> model is supported.
Please refer to OPENAI's documentation for more information.</p>
</details>
<details class="note">
<summary>Using LLAMA Embeddings</summary>
<p>The Llama embeddings are provided via the the python bindings of <code>llama.cpp</code>.
We do not provide the path to the quantized llama model. However it's easy to find and them online,
someone said there is a torrent/IPFS/direct download somewhere or llama weight, then you can quantized them yourself.
When the model weight are provided but are</p>
</details>
<details class="note">
<summary>Using Sentence Transformer Embeddings</summary>
<p>The sentence transformer embeddings are based on the SentenceTransformers package</p>
</details>


        <details class="quote">
          <summary>Source code in <code>molfeat_hype/trans/llm_embeddings.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LLMTransformer</span><span class="p">(</span><span class="n">PretrainedMolTransformer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Large Language Model Embeddings Transformer for molecule. This transformer embeds molecules using available LLM through langchain.</span>
<span class="sd">    Note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task.</span>
<span class="sd">    They are just trained on a large corpus of text.</span>

<span class="sd">    !!! warning &quot;Caching computation&quot;</span>
<span class="sd">        LLMs can be computationally expensive and even financially expensive if you use the OpenAI embeddings.</span>
<span class="sd">        To avoid recomputing the embeddings for the same molecules, we recommend using a molfeat Cache object.</span>
<span class="sd">        By default an in-memory  cache (DataCache) is used, but please explore other caching systems.</span>

<span class="sd">    ??? note &quot;Using OpenAI Embeddings&quot;</span>
<span class="sd">        If you are using the OpenAI embeddings, you need to either provide a &#39;open_ai_key&#39;</span>
<span class="sd">        argument or define one as an environment variable &#39;OPEN_AI_KEY&#39;.</span>
<span class="sd">        Please note that only the `text-embedding-ada-002` model is supported.</span>
<span class="sd">        Please refer to OPENAI&#39;s documentation for more information.</span>

<span class="sd">    ??? note &quot;Using LLAMA Embeddings&quot;</span>
<span class="sd">        The Llama embeddings are provided via the the python bindings of `llama.cpp`.</span>
<span class="sd">        We do not provide the path to the quantized llama model. However it&#39;s easy to find and them online,</span>
<span class="sd">        someone said there is a torrent/IPFS/direct download somewhere or llama weight, then you can quantized them yourself.</span>
<span class="sd">        When the model weight are provided but are</span>

<span class="sd">    ??? note &quot;Using Sentence Transformer Embeddings&quot;</span>
<span class="sd">        The sentence transformer embeddings are based on the SentenceTransformers package</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SUPPORTED_EMBEDDINGS</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;openai/text-embedding-ada-002&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span><span class="p">,</span>
        <span class="s2">&quot;llama.cpp&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">kind</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">],</span>
        <span class="n">standardize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">precompute_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">quantized_model_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parallel_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Instantiate a LLM Embeddings transformer</span>

<span class="sd">        Args:</span>
<span class="sd">            kind: kind of LLM to use. Supported LLMs are accessible through the SUPPORTED_EMBEDDINGS attribute. Here are a few:</span>
<span class="sd">                - &quot;openai/text-embedding-ada-002&quot;</span>
<span class="sd">                - &quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="sd">                - &quot;sentence-transformers/all-mpnet-base-v2&quot;</span>
<span class="sd">                - &quot;llama.cpp&quot;</span>
<span class="sd">                You can also provide any model hosted on hugginface that compute embeddings</span>
<span class="sd">            standardize: if True, standardize smiles before embedding</span>
<span class="sd">            precompute_cache: if True, add a cache to cache the embeddings for the same molecules.</span>
<span class="sd">            n_jobs: number of jobs to use for preprocessing smiles.</span>
<span class="sd">            dtype: data type to use for the embeddings return type</span>
<span class="sd">            openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</span>
<span class="sd">            quantized_model_path: path to the quantized model for llama.cpp. If None, will try to get it from the environment variable quantized_MODEL_PATH</span>
<span class="sd">            **params: parameters to pass to the LLM embeddings. See langchain documentation</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="p">)):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> not found, trying from huggingface hub.&quot;</span><span class="p">)</span>
                <span class="n">on_hgf</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;https://huggingface.co/</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">on_hgf</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Unknown LLM type </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> requested. Supported models are </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">openai_api_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">openai_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="n">kind</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">),</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;llama.cpp&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">quantized_model_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;QUANT_MODEL_PATH&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">quantized_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dm</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">):</span>
                    <span class="n">create_symlink</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">model_base_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">dm</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">dm</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CACHE_DIR</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_base_name</span><span class="si">}</span><span class="s2">*&quot;</span><span class="p">))</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Could not find the quantized model </span><span class="si">{</span><span class="n">model_base_name</span><span class="si">}</span><span class="s2"> anywhere, including in the cache dir </span><span class="si">{</span><span class="n">CACHE_DIR</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">quantized_model_path</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">redirect_stdout</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
                    <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">redirect_stderr</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
                        <span class="n">n_ctx</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_ctx&quot;</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="mi">1024</span><span class="p">)</span>
                        <span class="n">params</span><span class="p">[</span><span class="s2">&quot;n_ctx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_ctx</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">quantized_model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
                    <span class="n">model_name</span><span class="o">=</span><span class="n">kind</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cache_folder</span><span class="o">=</span><span class="n">CACHE_DIR</span>
                <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">precompute_cache</span><span class="o">=</span><span class="n">precompute_cache</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="n">parallel_kwargs</span><span class="o">=</span><span class="n">parallel_kwargs</span><span class="p">,</span>
            <span class="o">**</span><span class="n">params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">dm</span><span class="o">.</span><span class="n">Mol</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert the list of input molecules into the proper format for embeddings</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: list of input molecules</span>
<span class="sd">            **kwargs: additional keyword arguments for API consistency</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_preload</span><span class="p">()</span>
        <span class="n">parallel_kwargs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;parallel_kwargs&quot;</span><span class="p">,</span> <span class="p">{}))</span>
        <span class="n">parallel_kwargs</span><span class="p">[</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span>
        <span class="k">return</span> <span class="n">convert_smiles</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">parallel_kwargs</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smiles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This function takes a list of smiles or molecules and return the featurization</span>
<span class="sd">        corresponding to the inputs.</span>
<span class="sd">        In `transform` and `_transform`, this function is called after calling `_convert`</span>

<span class="sd">        Args:</span>
<span class="sd">            smiles: input smiles</span>
<span class="sd">            **kwargs: additional keyword arguments for API consistency</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_embeddings.LLMTransformer.SUPPORTED_EMBEDDINGS" class="doc doc-heading">
<code class="highlight language-python"><span class="n">SUPPORTED_EMBEDDINGS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;openai/text-embedding-ada-002&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence-transformers/all-mpnet-base-v2&#39;</span><span class="p">,</span> <span class="s1">&#39;llama.cpp&#39;</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.SUPPORTED_EMBEDDINGS" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_embeddings.LLMTransformer.kind" class="doc doc-heading">
<code class="highlight language-python"><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.kind" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_embeddings.LLMTransformer.model" class="doc doc-heading">
<code class="highlight language-python"><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">quantized_model_path</span><span class="p">,</span> <span class="kc">None</span><span class="o">=</span><span class="n">params</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.model" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_embeddings.LLMTransformer.standardize" class="doc doc-heading">
<code class="highlight language-python"><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.standardize" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>



<div class="doc doc-object doc-function">



<h4 id="molfeat_hype.trans.llm_embeddings.LLMTransformer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">],</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">precompute_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quantized_model_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parallel_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span></code>

<a href="#molfeat_hype.trans.llm_embeddings.LLMTransformer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Instantiate a LLM Embeddings transformer</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>kind</code></td>
          <td>
          </td>
          <td><p>kind of LLM to use. Supported LLMs are accessible through the SUPPORTED_EMBEDDINGS attribute. Here are a few:
- "openai/text-embedding-ada-002"
- "sentence-transformers/all-MiniLM-L6-v2"
- "sentence-transformers/all-mpnet-base-v2"
- "llama.cpp"
You can also provide any model hosted on hugginface that compute embeddings</p></td>
          <td>
                <code>Union[str, LangChainEmbeddings]</code>
          </td>
        </tr>
        <tr>
          <td><code>standardize</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>if True, standardize smiles before embedding</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>precompute_cache</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>if True, add a cache to cache the embeddings for the same molecules.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>n_jobs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of jobs to use for preprocessing smiles.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>dtype</code></td>
          <td>
          </td>
          <td><p>data type to use for the embeddings return type</p></td>
          <td>
                <code>float</code>
          </td>
        </tr>
        <tr>
          <td><code>openai_api_key</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>quantized_model_path</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>path to the quantized model for llama.cpp. If None, will try to get it from the environment variable quantized_MODEL_PATH</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**params</code></td>
          <td>
          </td>
          <td><p>parameters to pass to the LLM embeddings. See langchain documentation</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>molfeat_hype/trans/llm_embeddings.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">kind</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">],</span>
    <span class="n">standardize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">precompute_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">quantized_model_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parallel_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Instantiate a LLM Embeddings transformer</span>

<span class="sd">    Args:</span>
<span class="sd">        kind: kind of LLM to use. Supported LLMs are accessible through the SUPPORTED_EMBEDDINGS attribute. Here are a few:</span>
<span class="sd">            - &quot;openai/text-embedding-ada-002&quot;</span>
<span class="sd">            - &quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="sd">            - &quot;sentence-transformers/all-mpnet-base-v2&quot;</span>
<span class="sd">            - &quot;llama.cpp&quot;</span>
<span class="sd">            You can also provide any model hosted on hugginface that compute embeddings</span>
<span class="sd">        standardize: if True, standardize smiles before embedding</span>
<span class="sd">        precompute_cache: if True, add a cache to cache the embeddings for the same molecules.</span>
<span class="sd">        n_jobs: number of jobs to use for preprocessing smiles.</span>
<span class="sd">        dtype: data type to use for the embeddings return type</span>
<span class="sd">        openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</span>
<span class="sd">        quantized_model_path: path to the quantized model for llama.cpp. If None, will try to get it from the environment variable quantized_MODEL_PATH</span>
<span class="sd">        **params: parameters to pass to the LLM embeddings. See langchain documentation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="p">)):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> not found, trying from huggingface hub.&quot;</span><span class="p">)</span>
            <span class="n">on_hgf</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;https://huggingface.co/</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">on_hgf</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unknown LLM type </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> requested. Supported models are </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">openai_api_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">openai_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">kind</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">),</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;llama.cpp&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">quantized_model_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;QUANT_MODEL_PATH&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">quantized_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dm</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">):</span>
                <span class="n">create_symlink</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">,</span> <span class="n">kind</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_base_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">dm</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">dm</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CACHE_DIR</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_base_name</span><span class="si">}</span><span class="s2">*&quot;</span><span class="p">))</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">quantized_model_path</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Could not find the quantized model </span><span class="si">{</span><span class="n">model_base_name</span><span class="si">}</span><span class="s2"> anywhere, including in the cache dir </span><span class="si">{</span><span class="n">CACHE_DIR</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="n">quantized_model_path</span> <span class="o">=</span> <span class="n">quantized_model_path</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">redirect_stdout</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">redirect_stderr</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
                    <span class="n">n_ctx</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;n_ctx&quot;</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="mi">1024</span><span class="p">)</span>
                    <span class="n">params</span><span class="p">[</span><span class="s2">&quot;n_ctx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_ctx</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">quantized_model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
                <span class="n">model_name</span><span class="o">=</span><span class="n">kind</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cache_folder</span><span class="o">=</span><span class="n">CACHE_DIR</span>
            <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">precompute_cache</span><span class="o">=</span><span class="n">precompute_cache</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">parallel_kwargs</span><span class="o">=</span><span class="n">parallel_kwargs</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div><hr />
<h2 id="instruct-aware-embeddings">Instruct-aware Embeddings<a class="headerlink" href="#instruct-aware-embeddings" title="Permanent link">&para;</a></h2>
<p>This section corresponds to models that accept instructions to compute the embedding or an input molecule.</p>


<div class="doc doc-object doc-module">


<a id="molfeat_hype.trans.llm_instruct_embeddings"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="molfeat_hype.trans.llm_instruct_embeddings.DEFAULT_SYSTEM_PROMPT" class="doc doc-heading">
<code class="highlight language-python"><span class="n">DEFAULT_SYSTEM_PROMPT</span> <span class="o">=</span> <span class="s2">&quot;MolAssistant is a powerful large language model trained by the Open Drug Discovery consortium, </span><span class="se">\n</span><span class="s2">specifically designed to assist with various tasks related to drug discovery. Its capabilities range from providing </span><span class="se">\n</span><span class="s2">molecular representation to designing new molecules with specific optimization objectives in mind. Being pre-trained </span><span class="se">\n</span><span class="s2">on the largest corpus of chemical and biological data, MolAssistant has a deep understanding of chemical structures </span><span class="se">\n</span><span class="s2">and extensive biological knowledge. It can generate both human-like text and numerical outputs, providing concise and </span><span class="se">\n</span><span class="s2">accurate responses that are coherent with the topic and instructions given.</span><span class="se">\n\n</span><span class="s2">MolAssistant is constantly learning and improving, which allows it to process and understand vast amounts of chemical </span><span class="se">\n</span><span class="s2">and biological data, and it can comprehend any molecular structure. The model&#39;s knowledge enables it to provide valuable </span><span class="se">\n</span><span class="s2">insights and information for a wide range of tasks, including molecular search, QSAR model building, and molecular generation,</span><span class="se">\n</span><span class="s2">making it an indispensable tool for advancing the science of molecular modeling and design in drug discovery.</span><span class="se">\n\n</span><span class="si">{history}</span><span class="se">\n</span><span class="s2">Human: </span><span class="si">{human_input}</span><span class="se">\n</span><span class="s2">MolAssistant:&quot;</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module-attribute"><code>module-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.DEFAULT_SYSTEM_PROMPT" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="molfeat_hype.trans.llm_instruct_embeddings.MODEL_EMBEDDING_INSTRUCTIONS" class="doc doc-heading">
<code class="highlight language-python"><span class="n">MODEL_EMBEDDING_INSTRUCTIONS</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;instructor&#39;</span><span class="p">:</span> <span class="s1">&#39;Represent the following molecule for </span><span class="si">{context}</span><span class="s1">:&#39;</span><span class="p">,</span> <span class="s1">&#39;openai&#39;</span><span class="p">:</span> <span class="s1">&#39;I want you to provide </span><span class="si">{dimension}</span><span class="s1"> dimensional numerical vector at a precision of </span><span class="si">{precision}</span><span class="s1"> as a representation of molecules in the SMILES format that you will receive as input.</span><span class="se">\n</span><span class="s1">    You should first start by understanding the chemical structure and electronic properties of the input molecules before generating the </span><span class="si">{dimension}</span><span class="s1">-dimensional representation for the following task: </span><span class="si">{context}</span><span class="s1">.</span><span class="se">\n</span><span class="s1">    To obtain the output, I will provide you with either a single SMILES command or a list of SMILES commands, and you will reply with the most accurate and informative </span><span class="si">{dimension}</span><span class="s1">-dimensional representation in a json parseable format where the keys are the molecules and the values their representations.</span><span class="se">\n</span><span class="s1">    When generating the output, please ensure that the format is consistent with the task and the instruction given.  Do not write explanations. Do not type anything else unless I instruct you to do so. </span><span class="se">\n</span><span class="s1">    In case of any invalid or unrecognized SMILES inputs, please provide a suitable error message. My first molecule is c1ccccc1.&#39;</span><span class="p">}</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module-attribute"><code>module-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.MODEL_EMBEDDING_INSTRUCTIONS" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
  </div>

</div>


<div class="doc doc-object doc-class">



<h3 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer" class="doc doc-heading">
        <code>InstructLLMTransformer</code>


<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer" class="headerlink" title="Permanent link">&para;</a></h3>


  <div class="doc doc-contents ">
      <p class="doc doc-class-bases">
        Bases: <code><span title="molfeat.trans.pretrained.PretrainedMolTransformer">PretrainedMolTransformer</span></code></p>

  
      <p>Instruction-following Large Language Model Embeddings Transformer for molecules. This transformer embeds molecules using available LLM through langchain.
Note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task.
They are just trained on a large corpus of text.</p>


        <details class="quote">
          <summary>Source code in <code>molfeat_hype/trans/llm_instruct_embeddings.py</code></summary>
          <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">InstructLLMTransformer</span><span class="p">(</span><span class="n">PretrainedMolTransformer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instruction-following Large Language Model Embeddings Transformer for molecules. This transformer embeds molecules using available LLM through langchain.</span>
<span class="sd">    Note that the LLMs do not have any molecular context as they were not trained on molecules or any specific molecular task.</span>
<span class="sd">    They are just trained on a large corpus of text.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SUPPORTED_EMBEDDINGS</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;hkunlp/instructor-large&quot;</span><span class="p">,</span>
        <span class="s2">&quot;hkunlp/instructor-base&quot;</span><span class="p">,</span>
        <span class="s2">&quot;openai/gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="s2">&quot;openai/gpt-4&quot;</span><span class="p">,</span>
        <span class="s2">&quot;openai/chatgpt&quot;</span><span class="p">,</span>  <span class="c1"># alias for &quot;openai/gpt-3.5-turbo&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">kind</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">],</span>
        <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;modelling&quot;</span><span class="p">,</span>
        <span class="n">standardize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">precompute_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">conv_buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">conv_max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">system_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parallel_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Instantiate an instruction following LLM transformer for molecular embeddings</span>

<span class="sd">        Args:</span>
<span class="sd">            kind: type or name of the model to use for embeddings</span>
<span class="sd">            embedding_size: size of the embeddings to return for chat-like models</span>
<span class="sd">            context: context to give to the prompt for returning the results. Default is &quot;modelling&quot; which is the context for the modelling instructions.</span>
<span class="sd">            standardize: if True, standardize smiles before embedding</span>
<span class="sd">            precompute_cache: if True, add a cache to cache the embeddings for the same molecules.</span>
<span class="sd">            n_jobs: number of jobs to use for preprocessing smiles.</span>
<span class="sd">            conv_buffer_size: conversation buffer size so assistant can remember previous conversations and context for generating features.</span>
<span class="sd">            conv_max_tokens: maximum number of tokens to use for the conversation context. If None, will not use a token size limitations.</span>
<span class="sd">            dtype: data type to use for the embeddings return type</span>
<span class="sd">            openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</span>
<span class="sd">            precision: float precision of the output vector</span>
<span class="sd">            batch_size: batch size to use for embedding molecules. If None, will not use a batch size</span>
<span class="sd">            system_prompt: system prompt to use for chat-like models. If None, will use the default prompt.</span>
<span class="sd">            **params: parameters to pass to the LLM embeddings. See langchain documentation</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span> <span class="ow">or</span> <span class="s2">&quot;modelling&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_max_tokens</span> <span class="o">=</span> <span class="n">conv_max_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_buffer_size</span> <span class="o">=</span> <span class="n">conv_buffer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_length</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">system_prompt</span> <span class="o">=</span> <span class="n">system_prompt</span> <span class="ow">or</span> <span class="n">DEFAULT_SYSTEM_PROMPT</span>
        <span class="n">params</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
                <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="p">))</span> <span class="ow">or</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unknown LLM type </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> requested. Supported models are </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;openai/chatgpt&quot;</span><span class="p">:</span>
                    <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;openai/gpt-3.5-turbo&quot;</span>
                <span class="k">if</span> <span class="n">openai_api_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">openai_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
                <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
                    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;history&quot;</span><span class="p">,</span> <span class="s2">&quot;human_input&quot;</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">system_prompt</span>
                <span class="p">)</span>
                <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
                    <span class="n">model_name</span><span class="o">=</span><span class="n">kind</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">),</span>
                    <span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">params</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span>
                    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
                    <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">memory</span><span class="o">=</span><span class="n">EmbeddingConversationMemory</span><span class="p">(</span>
                        <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_buffer_size</span><span class="p">,</span>
                        <span class="n">ai_prefix</span><span class="o">=</span><span class="s2">&quot;MolAssistant&quot;</span><span class="p">,</span>
                        <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
                        <span class="n">max_token_limit</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_max_tokens</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                    <span class="n">human_input</span><span class="o">=</span><span class="n">MODEL_EMBEDDING_INSTRUCTIONS</span><span class="p">[</span><span class="s2">&quot;openai&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">dimension</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">embeddings</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;c1ccccc1&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Model is not able to understand the prompt. Please select a different model&quot;</span>
                    <span class="p">)</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;c1ccccc1&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="n">embedding_size</span>
                <span class="p">),</span> <span class="s2">&quot;Model cannot return the correct embedding size.&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_length</span> <span class="o">=</span> <span class="n">embedding_size</span>

            <span class="k">elif</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;llama.cpp&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gpt4all&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> is not yet supported, because or how slow they are.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># we need to remove temperature key</span>
                <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceInstructEmbeddings</span><span class="p">(</span>
                    <span class="n">model_name</span><span class="o">=</span><span class="n">kind</span><span class="p">,</span>
                    <span class="n">embed_instruction</span><span class="o">=</span><span class="n">MODEL_EMBEDDING_INSTRUCTIONS</span><span class="p">[</span><span class="s2">&quot;instructor&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span>
                    <span class="p">),</span>
                    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
                    <span class="n">cache_folder</span><span class="o">=</span><span class="n">CACHE_DIR</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">precompute_cache</span><span class="o">=</span><span class="n">precompute_cache</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="n">parallel_kwargs</span><span class="o">=</span><span class="n">parallel_kwargs</span><span class="p">,</span>
            <span class="o">**</span><span class="n">params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the length of the featurizer&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_length</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">dm</span><span class="o">.</span><span class="n">Mol</span><span class="p">]],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert the list of input molecules into the proper format for embeddings&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_preload</span><span class="p">()</span>
        <span class="n">parallel_kwargs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;parallel_kwargs&quot;</span><span class="p">,</span> <span class="p">{}))</span>
        <span class="n">parallel_kwargs</span><span class="p">[</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span>
        <span class="k">return</span> <span class="n">convert_smiles</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">parallel_kwargs</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">standardize</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smiles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;_embed takes a list of smiles or molecules and return the featurization</span>
<span class="sd">        corresponding to the inputs.  In `transform` and `_transform`, this function is</span>
<span class="sd">        called after calling `_convert`</span>

<span class="sd">        Args:</span>
<span class="sd">            smiles: input smiles</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span>
        <span class="c1"># basically running embeddings</span>
        <span class="c1"># compute expected total token for inputs based on expected number of char</span>
        <span class="n">expected_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">+</span> <span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">5</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">smiles</span>
        <span class="p">)</span>
        <span class="c1"># we splits the number of smiles to avoid being over the maximum tokens</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">maximum_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">max_tokens</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">max_token_limit</span> <span class="ow">or</span> <span class="mi">2000</span>
            <span class="n">n_splits</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">expected_tokens</span> <span class="o">/</span> <span class="n">maximum_tokens</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_splits</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">smiles</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)))</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">smiles</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Batch embedding&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">json_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">human_input</span><span class="o">=</span><span class="s2">&quot; ,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
            <span class="n">batch_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">json_output</span><span class="p">)</span>
            <span class="c1"># EN: surprisingly, ChatGPT can return randomized version of a SMILES</span>
            <span class="n">data</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">dm</span><span class="o">.</span><span class="n">unique_id</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">strip</span><span class="p">()):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch_data</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
        <span class="n">missed_embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dm</span><span class="o">.</span><span class="n">unique_id</span><span class="p">(</span><span class="n">sm</span><span class="p">),</span> <span class="n">missed_embedding</span><span class="p">)</span> <span class="k">for</span> <span class="n">sm</span> <span class="ow">in</span> <span class="n">smiles</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">data</span>
</code></pre></div></td></tr></table></div>
        </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.SUPPORTED_EMBEDDINGS" class="doc doc-heading">
<code class="highlight language-python"><span class="n">SUPPORTED_EMBEDDINGS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hkunlp/instructor-large&#39;</span><span class="p">,</span> <span class="s1">&#39;hkunlp/instructor-base&#39;</span><span class="p">,</span> <span class="s1">&#39;openai/gpt-3.5-turbo&#39;</span><span class="p">,</span> <span class="s1">&#39;openai/gpt-4&#39;</span><span class="p">,</span> <span class="s1">&#39;openai/chatgpt&#39;</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.SUPPORTED_EMBEDDINGS" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.batch_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.batch_size" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.context" class="doc doc-heading">
<code class="highlight language-python"><span class="n">context</span> <span class="o">=</span> <span class="n">context</span> <span class="ow">or</span> <span class="s1">&#39;modelling&#39;</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.context" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_buffer_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">conv_buffer_size</span> <span class="o">=</span> <span class="n">conv_buffer_size</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_buffer_size" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_max_tokens" class="doc doc-heading">
<code class="highlight language-python"><span class="n">conv_max_tokens</span> <span class="o">=</span> <span class="n">conv_max_tokens</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.conv_max_tokens" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.embedding_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.embedding_size" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.kind" class="doc doc-heading">
<code class="highlight language-python"><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.kind" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.model" class="doc doc-heading">
<code class="highlight language-python"><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.model" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.precision" class="doc doc-heading">
<code class="highlight language-python"><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.precision" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.standardize" class="doc doc-heading">
<code class="highlight language-python"><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.standardize" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.system_prompt" class="doc doc-heading">
<code class="highlight language-python"><span class="n">system_prompt</span> <span class="o">=</span> <span class="n">system_prompt</span> <span class="ow">or</span> <span class="n">DEFAULT_SYSTEM_PROMPT</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.system_prompt" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  </div>

</div>



<div class="doc doc-object doc-function">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">],</span> <span class="n">embedding_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="s1">&#39;modelling&#39;</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">precompute_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">conv_buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">conv_max_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">openai_api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">system_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">parallel_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span></code>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__init__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Instantiate an instruction following LLM transformer for molecular embeddings</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>kind</code></td>
          <td>
          </td>
          <td><p>type or name of the model to use for embeddings</p></td>
          <td>
                <code>Union[str, LangChainEmbeddings]</code>
          </td>
        </tr>
        <tr>
          <td><code>embedding_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>size of the embeddings to return for chat-like models</p></td>
          <td>
                <code>32</code>
          </td>
        </tr>
        <tr>
          <td><code>context</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>context to give to the prompt for returning the results. Default is "modelling" which is the context for the modelling instructions.</p></td>
          <td>
                <code>&#39;modelling&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>standardize</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>if True, standardize smiles before embedding</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>precompute_cache</code></td>
          <td>
                <code>bool</code>
          </td>
          <td><p>if True, add a cache to cache the embeddings for the same molecules.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>n_jobs</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>number of jobs to use for preprocessing smiles.</p></td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>conv_buffer_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>conversation buffer size so assistant can remember previous conversations and context for generating features.</p></td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>conv_max_tokens</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>maximum number of tokens to use for the conversation context. If None, will not use a token size limitations.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dtype</code></td>
          <td>
          </td>
          <td><p>data type to use for the embeddings return type</p></td>
          <td>
                <code>float</code>
          </td>
        </tr>
        <tr>
          <td><code>openai_api_key</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>precision</code></td>
          <td>
                <code>int</code>
          </td>
          <td><p>float precision of the output vector</p></td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[int]</code>
          </td>
          <td><p>batch size to use for embedding molecules. If None, will not use a batch size</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>system_prompt</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[str]</code>
          </td>
          <td><p>system prompt to use for chat-like models. If None, will use the default prompt.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>**params</code></td>
          <td>
          </td>
          <td><p>parameters to pass to the LLM embeddings. See langchain documentation</p></td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

      <details class="quote">
        <summary>Source code in <code>molfeat_hype/trans/llm_instruct_embeddings.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">kind</span><span class="o">=</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">LangChainEmbeddings</span><span class="p">],</span>
    <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;modelling&quot;</span><span class="p">,</span>
    <span class="n">standardize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">precompute_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">conv_buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">conv_max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
    <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">system_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">parallel_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Instantiate an instruction following LLM transformer for molecular embeddings</span>

<span class="sd">    Args:</span>
<span class="sd">        kind: type or name of the model to use for embeddings</span>
<span class="sd">        embedding_size: size of the embeddings to return for chat-like models</span>
<span class="sd">        context: context to give to the prompt for returning the results. Default is &quot;modelling&quot; which is the context for the modelling instructions.</span>
<span class="sd">        standardize: if True, standardize smiles before embedding</span>
<span class="sd">        precompute_cache: if True, add a cache to cache the embeddings for the same molecules.</span>
<span class="sd">        n_jobs: number of jobs to use for preprocessing smiles.</span>
<span class="sd">        conv_buffer_size: conversation buffer size so assistant can remember previous conversations and context for generating features.</span>
<span class="sd">        conv_max_tokens: maximum number of tokens to use for the conversation context. If None, will not use a token size limitations.</span>
<span class="sd">        dtype: data type to use for the embeddings return type</span>
<span class="sd">        openai_api_key: openai api key to use. If None, will try to get it from the environment variable OPENAI_API_KEY</span>
<span class="sd">        precision: float precision of the output vector</span>
<span class="sd">        batch_size: batch size to use for embedding molecules. If None, will not use a batch size</span>
<span class="sd">        system_prompt: system prompt to use for chat-like models. If None, will use the default prompt.</span>
<span class="sd">        **params: parameters to pass to the LLM embeddings. See langchain documentation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">kind</span> <span class="o">=</span> <span class="n">kind</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">standardize</span> <span class="o">=</span> <span class="n">standardize</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span> <span class="ow">or</span> <span class="s2">&quot;modelling&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">precision</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_max_tokens</span> <span class="o">=</span> <span class="n">conv_max_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv_buffer_size</span> <span class="o">=</span> <span class="n">conv_buffer_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_length</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">system_prompt</span> <span class="o">=</span> <span class="n">system_prompt</span> <span class="ow">or</span> <span class="n">DEFAULT_SYSTEM_PROMPT</span>
    <span class="n">params</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kind</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="p">))</span> <span class="ow">or</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown LLM type </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> requested. Supported models are </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">SUPPORTED_EMBEDDINGS</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="s2">&quot;openai/chatgpt&quot;</span><span class="p">:</span>
                <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;openai/gpt-3.5-turbo&quot;</span>
            <span class="k">if</span> <span class="n">openai_api_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">openai_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
                <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;history&quot;</span><span class="p">,</span> <span class="s2">&quot;human_input&quot;</span><span class="p">],</span> <span class="n">template</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">system_prompt</span>
            <span class="p">)</span>
            <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
                <span class="n">model_name</span><span class="o">=</span><span class="n">kind</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;openai/&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">),</span>
                <span class="n">openai_api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span>
                <span class="o">**</span><span class="n">params</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span>
                <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">memory</span><span class="o">=</span><span class="n">EmbeddingConversationMemory</span><span class="p">(</span>
                    <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_buffer_size</span><span class="p">,</span>
                    <span class="n">ai_prefix</span><span class="o">=</span><span class="s2">&quot;MolAssistant&quot;</span><span class="p">,</span>
                    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
                    <span class="n">max_token_limit</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_max_tokens</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">human_input</span><span class="o">=</span><span class="n">MODEL_EMBEDDING_INSTRUCTIONS</span><span class="p">[</span><span class="s2">&quot;openai&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">dimension</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;c1ccccc1&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Model is not able to understand the prompt. Please select a different model&quot;</span>
                <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;c1ccccc1&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="n">embedding_size</span>
            <span class="p">),</span> <span class="s2">&quot;Model cannot return the correct embedding size.&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_length</span> <span class="o">=</span> <span class="n">embedding_size</span>

        <span class="k">elif</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;llama.cpp&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kind</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gpt4all&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2"> is not yet supported, because or how slow they are.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># we need to remove temperature key</span>
            <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceInstructEmbeddings</span><span class="p">(</span>
                <span class="n">model_name</span><span class="o">=</span><span class="n">kind</span><span class="p">,</span>
                <span class="n">embed_instruction</span><span class="o">=</span><span class="n">MODEL_EMBEDDING_INSTRUCTIONS</span><span class="p">[</span><span class="s2">&quot;instructor&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span>
                <span class="p">),</span>
                <span class="n">model_kwargs</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
                <span class="n">cache_folder</span><span class="o">=</span><span class="n">CACHE_DIR</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">precompute_cache</span><span class="o">=</span><span class="n">precompute_cache</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">parallel_kwargs</span><span class="o">=</span><span class="n">parallel_kwargs</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>

<div class="doc doc-object doc-function">



<h4 id="molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#molfeat_hype.trans.llm_instruct_embeddings.InstructLLMTransformer.__len__" class="headerlink" title="Permanent link">&para;</a></h4>


  <div class="doc doc-contents ">
  
      <p>Get the length of the featurizer</p>

      <details class="quote">
        <summary>Source code in <code>molfeat_hype/trans/llm_instruct_embeddings.py</code></summary>
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the length of the featurizer&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_length</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
      </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright 2023 maclandrol
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.expand"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.51198bba.min.js"></script>
      
    
  </body>
</html>