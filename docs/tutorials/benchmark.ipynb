{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark the LLM representations against some common representations used in Molecular Modelling.  \n",
    "\n",
    "\n",
    "We will use the same setup found in the [molfeat benchmark](https://molfeat-docs.datamol.io/stable/benchmark.html). Note that this is not an extensive benchmark, and therefore the outcomes should not be taken as a definitive conclusion. \n",
    "\n",
    "In the molfeat benchmark, they used the following representations: **ECFP6**, **Mordred** and **ChemBERTa**. We will keep the same setup and will also use  their results to avoid rerunning the experiments. \n",
    "\n",
    "Furthermore, Because LLMs are computationally costly, we will only run the _Lipophilicity_ benchmark.\n",
    "\n",
    "In our experiments let's consider the following featurizers:\n",
    "\n",
    "- **openai/text-embedding-ada-002**: the default OpenAI embedding model\n",
    "- **sentence-transformers/all-mpnet-base-v2**: a popular [sentence embedding model](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) that maps text into a 768 dimensional dense vector.\n",
    "- **openai/gpt-3.5-turbo**: OpenAI instruction-following model that backs ChatGPT\n",
    "- **hkunlp/instructor-large**: an [instruction-conditioned model](https://huggingface.co/hkunlp/instructor-large) for embedding generation. \n",
    "\n",
    "<div class=\"admonition tip highlight\">\n",
    "<p class=\"admonition-title\">Tl;dr - Can non-finetuned LLMs outperform hand-crafted or pretrained molecular featurizers ?</p>\n",
    "<p>\n",
    "<strong>No !</strong> Understanding of molecular context/structure/properties is key for building good molecular featurizers. \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "! pip install auto-sklearn\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manu/.miniconda/envs/molfeat_hype/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datamol as dm\n",
    "import fsspec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import autosklearn.classification\n",
    "import autosklearn.regression\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from rdkit.Chem import SaltRemover\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from molfeat.utils.cache import FileCache\n",
    "from molfeat.trans.base import PrecomputedMolTransformer\n",
    "from molfeat.trans.fp import FPVecTransformer\n",
    "from molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer\n",
    "from molfeat_hype.trans.llm_embeddings import LLMTransformer\n",
    "from molfeat_hype.trans.llm_instruct_embeddings import InstructLLMTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the output less verbose\n",
    "warnings.simplefilter(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "dm.disable_rdkit_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(uri: str, readout_col: str):\n",
    "    \"\"\"Loads the MoleculeNet dataset\"\"\"\n",
    "    df = pd.read_csv(uri)\n",
    "    smiles = df[\"smiles\"].values\n",
    "    y = df[readout_col].values\n",
    "    return smiles, y\n",
    "\n",
    "\n",
    "def preprocess_smiles(smi):\n",
    "    \"\"\"Preprocesses the SMILES string\"\"\"\n",
    "    mol = dm.to_mol(smi, ordered=True, sanitize=False)    \n",
    "    try: \n",
    "        mol = dm.sanitize_mol(mol)\n",
    "    except:\n",
    "        mol = None\n",
    "            \n",
    "    if mol is None: \n",
    "        return\n",
    "        \n",
    "    mol = dm.standardize_mol(mol, disconnect_metals=True)\n",
    "    remover = SaltRemover.SaltRemover()\n",
    "    mol = remover.StripMol(mol, dontRemoveEverything=True)\n",
    "\n",
    "    return dm.to_smiles(mol)\n",
    "\n",
    "\n",
    "def scaffold_split(smiles):\n",
    "    \"\"\"In line with common practice, we will use the scaffold split to evaluate our models\"\"\"\n",
    "    scaffolds = [dm.to_smiles(dm.to_scaffold_murcko(dm.to_mol(smi))) for smi in smiles]\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    return next(splitter.split(smiles, groups=scaffolds))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ada_cache = FileCache(cache_file=\"../../cache/openai_ada_cache.parquet\", name=\"openai_ada_cache\")\n",
    "transf_openai_ada = LLMTransformer(kind=\"openai/text-embedding-ada-002\", openai_api_key=openai_api_key, precompute_cache=openai_ada_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_trans_cache = FileCache(cache_file=\"../../cache/sentence_transformer.parquet\", name=\"sent_trans_cache\")\n",
    "transf_sentence = LLMTransformer(kind=\"sentence-transformers/all-mpnet-base-v2\", precompute_cache=sent_trans_cache)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruct embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "cond_embed_cache = FileCache(cache_file=\"../../cache/cond_embed.parquet\", name=\"cond_embed_cache\")\n",
    "transf_cond_embed = InstructLLMTransformer(kind=\"hkunlp/instructor-large\", precompute_cache=cond_embed_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lipophilicity\n",
    "Lipophilicity is a regression task with 4200 molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Lipophilicity dataset\n",
    "smiles, y_true = load_dataset(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\", \"exp\")\n",
    "\n",
    "smiles = np.array([preprocess_smiles(smi) for smi in smiles])\n",
    "smiles = np.array([smi for smi in smiles if dm.to_mol(smi) is not None])\n",
    "\n",
    "feats_openai_ada, ind_openai_ada = transf_openai_ada(smiles, ignore_errors=True)\n",
    "\n",
    "X = {\n",
    "    \"openai/text-embedding-ada-002\": feats_openai_ada[ind_openai_ada],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_sentence, ind_sentence = transf_sentence(smiles, ignore_errors=True)\n",
    "X[\"sentence-transformers/all-mpnet-base-v2\"] = feats_sentence[ind_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats_cond_embed = transf_cond_embed.batch_transform(transf_cond_embed, smiles, batch_size=512, n_jobs=8, progress=True)\n",
    "# ind_cond_embed = np.arange(len(smiles))\n",
    "# X[\"hkunlp/instructor-large\"] = feats_cond_embed[ind_cond_embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mols = [dm.to_mol(smi) for smi in smiles]\n",
    "# _cache = dict(zip(mols, feats_cond_embed))\n",
    "# transf_cond_embed.precompute_cache.update(_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_cond_embed, ind_cond_embed = transf_cond_embed(smiles, ignore_errors=True)\n",
    "X[\"hkunlp/instructor-large\"] = feats_cond_embed[ind_cond_embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../../cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transf_base_chatgpt = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16, openai_api_key=openai_api_key, precompute_cache=False, conv_buffer_size=4, request_timeout=300)\n",
    "# transf_chatgpt = PrecomputedMolTransformer(cache=chatgpt_cache, featurizer=transf_base_chatgpt)\n",
    "#feats_chatgpt = transf_chatgpt.batch_transform(transf_chatgpt, smiles, batch_size=16, n_jobs=-1)\n",
    "#X[\"openai/chatgpt\"] = feats_chatgpt[ind_chatgpt]\n",
    "# chatgpt_cache.update(transf_chatgpt.cache)\n",
    "# chatgpt_cache.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_cache = FileCache(cache_file=\"../../cache/chatgpt.parquet\", name=\"chatgpt_cache\")\n",
    "transf_chatgpt = InstructLLMTransformer(kind=\"openai/chatgpt\", embedding_size=16, openai_api_key=openai_api_key, conv_buffer_size=3, request_timeout=300, precompute_cache=chatgpt_cache, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, x in transf_chatgpt.precompute_cache.cache.copy().items():\n",
    "#     if x is None  or np.any(np.isnan(x)):\n",
    "#         del transf_chatgpt.precompute_cache.cache[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_chatgpt, ind_chatgpt = transf_chatgpt(smiles, ignore_errors=True)\n",
    "X[\"openai/chatgpt\"] = feats_chatgpt#[ind_chatgpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_sentence.precompute_cache.save_to_file()\n",
    "transf_openai_ada.precompute_cache.save_to_file()\n",
    "transf_cond_embed.precompute_cache.save_to_file()\n",
    "transf_chatgpt.precompute_cache.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2023-05-01 03:59:27,939:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 03:59:47,153:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:00:06,341:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:02:17,152:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:02:36,289:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:05:11,269:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:05:30,394:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:08:03,297:Client-EnsembleBuilder] No runs were available to build an ensemble from\n",
      "[WARNING] [2023-05-01 04:08:09,072:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:08:28,220:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:08:31,846:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:08:50,975:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:09:10,108:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:09:29,307:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:09:33,133:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:09:34,807:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:09:37,968:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:09:57,130:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:10:16,333:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n",
      "[WARNING] [2023-05-01 04:10:32,489:Client-EnsembleBuilder] No models better than random - using Dummy losses!\n",
      "\tModels besides current dummy model: 0\n",
      "\tDummy models: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'openai/text-embedding-ada-002': 0.8497511191899347,\n",
       " 'sentence-transformers/all-mpnet-base-v2': 0.846618632301065,\n",
       " 'hkunlp/instructor-large': 0.8109557972763406,\n",
       " 'openai/chatgpt': 0.9238916528125248}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model\n",
    "train_ind, test_ind = scaffold_split(smiles)\n",
    "\n",
    "lipo_scores = {}\n",
    "for name, feats in X.items():\n",
    "    # print(name, feats.shape, y_true.shape, np.any(np.isnan(feats)))\n",
    "    # Train\n",
    "    automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "        memory_limit=None, \n",
    "        # For practicality’s sake, limit this to 5 minutes! \n",
    "        # (x3 = 15 min in total)\n",
    "        time_left_for_this_task=180,  \n",
    "        n_jobs=1,\n",
    "        seed=1,\n",
    "    )\n",
    "    automl.fit(feats[train_ind], y_true[train_ind])\n",
    "    \n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_hat = automl.predict(feats[test_ind])\n",
    "    \n",
    "    # Evaluate\n",
    "    mae = mean_absolute_error(y_true[test_ind], y_hat)\n",
    "    lipo_scores[name] = mae\n",
    "\n",
    "lipo_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "| Dataset       | Metric   | Representation | Score | Rank |\n",
    "|---------------|----------|----------------|-------|------|\n",
    "| Lipophilicity | MAE ↓    | ECFP           | 0.727  | 1    |\n",
    "|               |          | Mordred        | 0.579  | 0    |\n",
    "|               |          | ChemBERTa      | 0.740  | 2    |\n",
    "|               |          | openai/text-embedding-ada-002      | 0.850  | 5    |\n",
    "|               |          | sentence-transformers/all-mpnet-base-v2      | 0.847  | 4    |\n",
    "|               |          | hkunlp/instructor-large      | 0.811  | 3    |\n",
    "|               |          | openai/chatgpt      | 0.924  | 6    |\n",
    "\n",
    "\n",
    "Without surprise, the models built with LLMs embeddings without any finetuning performed worse than the molecular structure/context/properties aware featurization and some of them were not better than random models. Interestingly, the instruction conditioned embedding performed ok, while ChatGPT was the worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd64925fe6617865d410306d2b64fa69b44b63a36aad85fd11f7d4e4dc7609f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
